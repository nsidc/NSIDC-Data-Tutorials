{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e86eaecf-a612-4dbb-8bdc-5b5dfddf65b9",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src='./img/nsidc_logo.png'/>\n",
    "\n",
    "# **Using Coiled and h5coro to Produce ICESat-2 Sea Ice Height Time Series**\n",
    "\n",
    "</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4101ae06-3984-435c-abcc-f6346d15069b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **1. Tutorial Introduction/Overview**\n",
    "\n",
    "Tutorial designed for the \"DAAC data access in the cloud hands-on experience\" session at the 2023 NSIDC DAAC User Working Group (UWG) Meeting. This is a copy of the `2_ATL07_timeseries` notebook for use with Coiled.\n",
    "\n",
    "\n",
    "TODOS:\n",
    "* Explain Coiled\n",
    "* Question for Luis: Why would I use the decorator function (` @coiled.function()`) vs:\n",
    "\n",
    "```\n",
    "cluster = coiled.Cluster(n_workers=20, region=\"us-west-2\")\n",
    "client = cluster.get_client()\n",
    "client\n",
    "```\n",
    "* How do we incorporate https://medium.com/coiled-hq/processing-a-250-tb-dataset-with-coiled-dask-and-xarray-574370ba5bde ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e689b48a-89e1-44c9-a681-75a669286bb1",
   "metadata": {},
   "source": [
    "### Installing last versions from earthaccess and coiled\n",
    "\n",
    "**NOTE**: Restart the kernel and clean output after the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9700d441-441a-41fb-9ad8-7ea5eabec52b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install coiled==0.9.26\n",
    "\n",
    "!pip uninstall earthaccess\n",
    "!pip install git+https://github.com/jrbourbeau/earthaccess.git@pickle-logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b77eb5-d5ed-4ddd-8fb1-6c69618d7852",
   "metadata": {},
   "source": [
    "## **2. Tutorial steps**\n",
    "\n",
    "Resoruces: each granule is approx 60-120 MB, A month of data for the Ross ocean returns 59 granules ~4.6 GB. We should use an instance preferable double the memory of the aprox data size we use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820a737-33f0-4470-b9a4-03c5c4f0354c",
   "metadata": {},
   "source": [
    "### **Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e79729-1b02-4ef5-aee1-8923690243da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For Coiled cloud compute\n",
    "import coiled\n",
    "\n",
    "# For searching NASA data\n",
    "import earthaccess\n",
    "\n",
    "# For reading data, analysis and plotting\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import hvplot.xarray\n",
    "\n",
    "import pprint\n",
    "from affine import Affine\n",
    "from pyproj import CRS\n",
    "\n",
    "from pqdm.threads import pqdm\n",
    "\n",
    "print(coiled.__version__)\n",
    "print(earthaccess.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1966ffa6-a5f2-4520-a8dc-f37678a2cf7a",
   "metadata": {},
   "source": [
    "### **Authenticate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47aa955-3d91-4418-85f9-5772f400f712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auth = earthaccess.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da19f604-0288-4358-ab88-d18c986f7cc8",
   "metadata": {},
   "source": [
    "### **Search for ICESat-2 ATL07 data**\n",
    "\n",
    "Using spatial/temporal range from https://icesat-2-2023.hackweek.io/tutorials/sea_ice/1_sea_ice_tutorial.html :\n",
    "\n",
    "\n",
    "```\n",
    "# Spatial extent: Ross Sea, Antarctica\n",
    "spatial_extent = [-180, -78, -160, -74]\n",
    "\n",
    "# Time range\n",
    "date_range = ['2019-09-16','2019-09-16'] # first time period\n",
    "# date_range = ['2019-11-13','2019-11-13'] # second time period\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61875e91-c8b4-4beb-9535-c3391d1fcc06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "atl10 = {}\n",
    "total_results = 0\n",
    "\n",
    "for year in range(2019,2020):\n",
    "    \n",
    "    print(f\"Searching year {year} ...\")\n",
    "    granules = earthaccess.search_data(\n",
    "        short_name = 'ATL10',\n",
    "        version = '006',\n",
    "        cloud_hosted = True,\n",
    "        bounding_box = (-180, -78, -160, -74),\n",
    "        temporal = (f'{year}-09-01',f'{year}-09-30'),\n",
    "    )\n",
    "    total_results += len(granules)\n",
    "    atl10[str(year)] = granules\n",
    "print(f\"Total: {total_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aacb907-e255-4c60-9eb8-d00c552685f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = [display(r) for r in atl10[\"2019\"][0:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6887297-2b02-4e76-9bc5-5e804c54c5d7",
   "metadata": {},
   "source": [
    "### **Extract freeboard segments**\n",
    "\n",
    "We now create a geopandas dataset from our results. \n",
    "\n",
    "Because ATL10 is not a gridded prduct we need to extract coordinates and variables from their groups inside the HDF5 file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ff0fab-e1ad-4426-b7eb-e8e4f048b144",
   "metadata": {},
   "source": [
    "#### Open the files using the `open` method. \n",
    "\n",
    "The auth object created at the start of the notebook is used to provide Earthdata Login authentication and AWS credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0142dd34-21f5-48ac-825e-328e772ecaa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_tree = {}\n",
    "\n",
    "for year, granules in atl10.items():\n",
    "    file_tree[year] = earthaccess.open(granules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636d273-25e6-451f-931f-c5a7ec52d721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# files[0].f.s3.storage_options\n",
    "print(file_tree[\"2019\"][0].f.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b5e090-1747-4cfc-bbd5-815e5df4020a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(file_tree[\"2019\"][0],'r') as f:\n",
    "\n",
    "    time = f[\"gt1r\"]['freeboard_segment/delta_time'][:]\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc9d0fe-9184-4f20-ac6c-38c98bcd88c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(file_tree[\"2019\"][0], group=\"gt1r/freeboard_segment/\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37de57-f225-4d50-8ac2-8d9b7b76d9f7",
   "metadata": {},
   "source": [
    "### Pre-warming the Coiled instance.\n",
    "\n",
    "Once we get to run this with Coiled it would be good to instantiate the cluster beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d605657-3cb8-4894-9adc-45f020d6ed41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @coiled.function(region=\"us-west-2\",\n",
    "#                  memory=\"16 GiB\")\n",
    "# def trivial(param):\n",
    "#     print(param)\n",
    "#     return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f58418-0c0e-49ca-adfa-d3fe6b53c4d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trivial(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270e1930-1908-48eb-a1a5-831fba89009a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Based on the READ function form Younghyun Koo for the sea ice tutorial at the IS2 hackweek\n",
    "\n",
    "# @coiled.function(region=\"us-west-2\",\n",
    "#                  memory=\"16 GiB\")\n",
    "def read_atl10_local(files, executors):\n",
    "    \"\"\"Returns a consolidated GeoPandas dataframe for a set of ATL10 file pointers.\n",
    "    \n",
    "    Parameters:\n",
    "        files (list[S3FSFile]): list of authenticated fsspec file references to ATL10 on S3 (via earthaccess)\n",
    "        executors (int): number of threads\n",
    "    \n",
    "    \"\"\"\n",
    "    from h5coro import h5coro, s3driver, filedriver\n",
    "    from itertools import product\n",
    "    import geopandas as gpd\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import gc\n",
    "    \n",
    "    def read_atl10(file):\n",
    "        # Create a list for saving ATL10 beam track data\n",
    "        tracks = []\n",
    "        credentials = {\"aws_access_key_id\": file.s3.storage_options[\"key\"],\n",
    "                       \"aws_secret_access_key\": file.s3.storage_options[\"secret\"],\n",
    "                       \"aws_session_token\": file.s3.storage_options[\"token\"]}\n",
    "        \n",
    "        f = h5coro.H5Coro(file.info()[\"name\"], s3driver.S3Driver, credentials=credentials)\n",
    "        f.readDatasets(datasets=[\"orbit_info/sc_orient\"], block=True)\n",
    "        \n",
    "        # Check the orbit orientation\n",
    "        orient = f['orbit_info/sc_orient'][0]\n",
    "\n",
    "        if orient == 0:\n",
    "            strong_beams = [f\"gt{i}l\" for i in [1, 2, 3]]\n",
    "        elif orient == 1:\n",
    "            strong_beams = [f\"gt{i}r\" for i in [1, 2, 3]]\n",
    "        else:\n",
    "            strong_beams = []\n",
    "            \n",
    "            \n",
    "        datasets = [\"freeboard_segment/latitude\",\n",
    "                    \"freeboard_segment/longitude\",\n",
    "                    \"freeboard_segment/delta_time\",\n",
    "                    \"freeboard_segment/seg_dist_x\",\n",
    "                    \"freeboard_segment/heights/height_segment_length_seg\",\n",
    "                    \"freeboard_segment/beam_fb_height\",\n",
    "                    \"freeboard_segment/heights/height_segment_type\"]\n",
    "            \n",
    "        ds_list = [\"/\".join(p) for p in list(product(strong_beams, datasets))]\n",
    "        f.readDatasets(datasets=ds_list, block=True)\n",
    "        \n",
    "        # not taking into account 37 leap seconds\n",
    "        gps_epoch = pd.to_datetime('1980-01-06 00:00:00')\n",
    "    \n",
    "        for beam in strong_beams:\n",
    "            lat = f[f'{beam}/freeboard_segment/latitude'][:]\n",
    "            lon = f[f'{beam}/freeboard_segment/longitude'][:]\n",
    "            gps_since_epoch = f[f'{beam}/freeboard_segment/delta_time'][:]\n",
    "            seg_x = f[f'{beam}/freeboard_segment/seg_dist_x'][:] / 1000 # (m to km)\n",
    "            seg_len = f[f'{beam}/freeboard_segment/heights/height_segment_length_seg'][:]\n",
    "            fb = f[f'{beam}/freeboard_segment/beam_fb_height'][:]\n",
    "            surface_type = f[f'{beam}/freeboard_segment/heights/height_segment_type'][:]\n",
    "            fb[fb>100] = np.nan\n",
    "            \n",
    "            # ATL10 ATB\n",
    "            is2_epoch = 1.1988e+9\n",
    "            \n",
    "            date_time = gps_epoch + pd.to_timedelta(gps_since_epoch+is2_epoch, unit='s')\n",
    "\n",
    "            df = pd.DataFrame({'lat': lat, 'lon': lon, 'time': date_time, 'seg_x': seg_x, 'seg_len': seg_len,\n",
    "                              'freeboard': fb, 'stype': surface_type})\n",
    "            df['beam'] = beam\n",
    "            df = df.dropna().reset_index(drop = True)\n",
    "            gdf = gpd.GeoDataFrame(\n",
    "                    df, geometry=gpd.points_from_xy(df.lon, df.lat), crs=\"EPSG:4326\"\n",
    "            )\n",
    "            del gdf[\"lat\"]\n",
    "            del gdf[\"lon\"]\n",
    "\n",
    "            gc.collect()\n",
    "            tracks.append(gdf)\n",
    "        # print(f\"Done with {file.info()['name']}\")\n",
    "        return tracks\n",
    "    df = pqdm(files, read_atl10, n_jobs=executors)\n",
    "    combined = pd.concat([t[0] for t in df if type(t) is list])\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51ffca4-1c56-4659-a255-1cd38c86484d",
   "metadata": {},
   "source": [
    "The idea would be to split each year into its own Dask worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc14401-66a6-44ba-b8f2-421f45e50c29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tracks = read_atl10_local(file_tree[\"2019\"], executors=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f577fc-7c75-456e-b7bb-2a4f45ab77c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f17a22-5578-4dfb-a474-218741ac8a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tracks.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27844c4b-bdb2-431b-9c85-8afd5ed09f0d",
   "metadata": {},
   "source": [
    "### For future IO eficient operations we save the geodataframe as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47292a7-25e7-43a5-8c45-9fecfdb21c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tracks.to_parquet(\"atl10-2019.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6112e199-cd11-4f80-bc4b-4682035e3d81",
   "metadata": {},
   "source": [
    "#### Geopandas Read function \n",
    "\n",
    "The function below extracts latitude, longitude, segment distance, segment length, surface type, and freeboard height. See the [NSIDC's ATL10 User Guide](https://nsidc.org/sites/default/files/documents/user-guide/atl10-v006-userguide.pdf) for more details on these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63674b7-c92a-4bc1-818c-9dae0cf9cc69",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Calculate grid indices of segment centers**\n",
    "\n",
    "Using pyproj and Affine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92549f7-1bb0-4373-b810-1813d450fb31",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Assign to grid and calculate grid cell mean**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b2ef6-3e14-475e-b689-77bda4c1814e",
   "metadata": {},
   "source": [
    "## **3. Learning outcomes recap (optional)**\n",
    "\n",
    "Provide a brief summary of the learning outcomes of the tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87da360-fa68-43e6-993e-5d5098b70aed",
   "metadata": {},
   "source": [
    "## **4. Additional resources (optional)**\n",
    "\n",
    "List some additional resources for users to consult, if applicable/desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178b3e1e-71e4-4aab-bdbe-4d33dbce01e9",
   "metadata": {},
   "source": [
    "________\n",
    "\n",
    "### **When your tutorial is ready for review,  please read our [Contributor Guide](https://github.com/nsidc/NSIDC-Data-Tutorials/blob/main/contributor_guide.md) for next steps.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7019c-b0c8-4c96-a36c-9cb1d17b6303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
