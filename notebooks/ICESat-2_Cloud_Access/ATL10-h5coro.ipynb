{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e86eaecf-a612-4dbb-8bdc-5b5dfddf65b9",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "<center>\n",
    "<img src='./img/nsidc_logo.png'/>\n",
    "\n",
    "# **Using Coiled and h5coro to Produce ICESat-2 Sea Ice Height Time Series**\n",
    "\n",
    "</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4101ae06-3984-435c-abcc-f6346d15069b",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## **1. Tutorial Introduction/Overview**\n",
    "\n",
    "_@amy I don't think we need to mention coiled here.  This notebook just describe a cloud-based workflow.  The script is where coiled comes in._\n",
    "\n",
    "Tutorial designed for the \"DAAC data access in the cloud hands-on experience\" session at the 2023 NSIDC DAAC User Working Group (UWG) Meeting. This is a copy of the `2_ATL07_timeseries` notebook for use with Coiled.\n",
    "\n",
    "\n",
    "TODOS:\n",
    "* Explain Coiled\n",
    "* Question for Luis: Why would I use the decorator function (` @coiled.function()`) vs:\n",
    "\n",
    "```\n",
    "cluster = coiled.Cluster(n_workers=20, region=\"us-west-2\")\n",
    "client = cluster.get_client()\n",
    "client\n",
    "```\n",
    "* How do we incorporate https://medium.com/coiled-hq/processing-a-250-tb-dataset-with-coiled-dask-and-xarray-574370ba5bde ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e689b48a-89e1-44c9-a681-75a669286bb1",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Installing last versions from earthaccess\n",
    "\n",
    "_@luis: do we still need to do this step?_\n",
    "\n",
    "**NOTE**: Restart the kernel and clean output after the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9700d441-441a-41fb-9ad8-7ea5eabec52b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "!pip uninstall -y earthaccess\n",
    "!pip install git+https://github.com/nsidc/earthaccess.git@main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b77eb5-d5ed-4ddd-8fb1-6c69618d7852",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## **2. Tutorial steps**\n",
    "\n",
    "Resoruces: each granule is approx 60-120 MB, A month of data for the Ross ocean returns 59 granules ~4.6 GB. We should use an instance preferable double the memory of the aprox data size we use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820a737-33f0-4470-b9a4-03c5c4f0354c",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### **Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59e79729-1b02-4ef5-aee1-8923690243da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earthaccess: 0.6.1\n"
     ]
    }
   ],
   "source": [
    "# To force use of shapely\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "# For searching NASA data\n",
    "import earthaccess\n",
    "\n",
    "# For reading data, analysis and plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For resampling\n",
    "from affine import Affine\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "from readers import read_atl10_local\n",
    "\n",
    "print(f\"earthaccess: {earthaccess.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1966ffa6-a5f2-4520-a8dc-f37678a2cf7a",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Authenticate\n",
    "\n",
    "We need to authenticate and get AWS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d47aa955-3d91-4418-85f9-5772f400f712",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARTHDATA_USERNAME and EARTHDATA_PASSWORD are not set in the current environment, try setting them or use a different strategy (netrc, interactive)\n",
      "You're now authenticated with NASA Earthdata Login\n",
      "Using token with expiration date: 11/18/2023\n",
      "Using .netrc file for EDL\n"
     ]
    }
   ],
   "source": [
    "auth = earthaccess.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da19f604-0288-4358-ab88-d18c986f7cc8",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### **Search for ICESat-2 ATL07 data**\n",
    "\n",
    "We use `earthaccess` to search CMR for granules in the region of interest for the time period of interest.  \n",
    "\n",
    "The region is set by name below.  Currently, we have two options: the Ross Sea, and the Southern Ocean and adjoining seas.\n",
    "\n",
    "The range of dates is set by assigning a start year and end year to `year_begin` and `year_end`.  Setting `year_begin` and `year_end` to the same year retreives data for one year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2069528-d382-45ab-a865-a41593bc47a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To avoid copying and pasting region tuples\n",
    "region = \"Ross Sea\"  # Set region to \"Ross Sea\" for just Ross Sea or \"Antarctica\" for southern ocean \n",
    "ross_sea = (-180, -78, -160, -74)\n",
    "antarctic = (-180, -90, 180, -60)\n",
    "this_region = antarctic if region == \"Antarctica\" else ross_sea\n",
    "\n",
    "year_begin = 2019\n",
    "year_end = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61875e91-c8b4-4beb-9535-c3391d1fcc06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching year 2019 ...\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/requests/models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/simplejson/__init__.py:514\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, allow_nan, **kw)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    511\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    512\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_decimal \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/simplejson/decoder.py:386\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w, _PY3)\u001b[0m\n\u001b[1;32m    385\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(s, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n\u001b[0;32m--> 386\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/simplejson/decoder.py:416\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx, _w, _PY3)\u001b[0m\n\u001b[1;32m    415\u001b[0m         idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m--> 416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(year_begin, year_end\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearching year \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     granules \u001b[38;5;241m=\u001b[39m \u001b[43mearthaccess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshort_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mATL10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m006\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcloud_hosted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbounding_box\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthis_region\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemporal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43myear\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-09-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43myear\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-09-30\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     total_results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(granules)\n\u001b[1;32m     15\u001b[0m     atl10[\u001b[38;5;28mstr\u001b[39m(year)] \u001b[38;5;241m=\u001b[39m granules\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/earthaccess/api.py:106\u001b[0m, in \u001b[0;36msearch_data\u001b[0;34m(count, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Search dataset granules using NASA's CMR.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m[https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html](https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m earthaccess\u001b[38;5;241m.\u001b[39m__auth__\u001b[38;5;241m.\u001b[39mauthenticated:\n\u001b[0;32m--> 106\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[43mDataGranules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mearthaccess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__auth__\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     query \u001b[38;5;241m=\u001b[39m DataGranules()\u001b[38;5;241m.\u001b[39mparameters(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/earthaccess/search.py:347\u001b[0m, in \u001b[0;36mDataGranules.parameters\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m         methods[key](\u001b[38;5;241m*\u001b[39mval)\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m         \u001b[43mmethods\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/earthaccess/search.py:408\u001b[0m, in \u001b[0;36mDataGranules.cloud_hosted\u001b[0;34m(self, cloud_hosted)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcloud_hosted must be of type bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshort_name\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams:\n\u001b[0;32m--> 408\u001b[0m     provider \u001b[38;5;241m=\u001b[39m \u001b[43mfind_provider_by_shortname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshort_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcloud_hosted\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m provider \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovider\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m provider\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/earthaccess/daac.py:155\u001b[0m, in \u001b[0;36mfind_provider_by_shortname\u001b[0;34m(short_name, cloud_hosted)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_provider_by_shortname\u001b[39m(short_name: \u001b[38;5;28mstr\u001b[39m, cloud_hosted: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m    152\u001b[0m     base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://cmr.earthdata.nasa.gov/search/collections.umm_json?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m     providers \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m&cloud_hosted=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcloud_hosted\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m&short_name=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mshort_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m--> 155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(providers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m providers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovider-id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/requests/models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m--> 975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "atl10 = {}\n",
    "total_results = 0\n",
    "\n",
    "for year in range(year_begin, year_end+1):\n",
    "    \n",
    "    print(f\"Searching year {year} ...\")\n",
    "    granules = earthaccess.search_data(\n",
    "        short_name = 'ATL10',\n",
    "        version = '006',\n",
    "        cloud_hosted = True,\n",
    "        bounding_box = this_region,\n",
    "        temporal = (f'{year}-09-01',f'{year}-09-30'),\n",
    "    )\n",
    "    total_results += len(granules)\n",
    "    atl10[str(year)] = granules\n",
    "print(f\"Total: {total_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6887297-2b02-4e76-9bc5-5e804c54c5d7",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Access the Granules\n",
    "\n",
    "Because the CryoCloud is hub is running on servers in AWS region `us-west-2`, which is the same region as the NASA Earthdata Cloud, granules can be accessed directly without having to download the files first.  This is analogous to how you would work with files on your local filesystem.  However, _under the hood_ there are differences.\n",
    "\n",
    "Initially, we load data for each year into a `geopandas.DataFrame`.  `geopandas` is an extension of the `pandas` package.  `pandas` is designed to work with `tabular` data - _think data you might put into a spreadsheet_.  `geopandas`, extends `pandas` to work with geospatial data by adding geometries (points, lines and polygons) and a coordinate reference system (CRS), so that data in each row is associated with a geospatial feature located on Earth.  ICESat-2 track data is well suited to the DataFrame data model because data are related to points or segments.  Once data is in a `geopandas.DataFrame`, the data can be reprojected and queried using methods you may be used to using in a GIS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ff0fab-e1ad-4426-b7eb-e8e4f048b144",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Open the files\n",
    "\n",
    "The first step is to open the files using the `earthaccess.open` method.   So that we can work with files by year, we create a dictionary organized by year that contains _file-like_ objects.  At this point, we haven't read what is in the files, we've just opened them ready for reading.\n",
    "\n",
    "```{warning}\n",
    "We do not need to do this for h5coro\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0142dd34-21f5-48ac-825e-328e772ecaa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_tree = {}\n",
    "for year, granules in atl10.items():\n",
    "    file_tree[year] = earthaccess.open(granules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80340da9-1b3b-458d-9d94-9492657f94bf",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Read data into `geopandas.DataFrame`\n",
    "\n",
    "The next step is to read the data and put it into a Dataframe.  We use `h5coro`, which is a package developed by the SlideRule project to efficiently read HDF5 files in the cloud.  Recall from the Cloud Optimized Format presentation, the HDF5 format and the HDF5 library for reading and writing those files are not well suited to accessing data in the cloud.  `h5coro` was developed to solve some of the problems related to HDF5 format and tools.  Using `h5coro` with `dask`, a python package for parallel processing on multicore local machines and distributed cluster in the cloud, reading data from ATL10 files is 5x faster than using the `h5py` package, an HDF5 reader that uses the HDF5 library.\n",
    "\n",
    "The code to read the data is long, so we have created the `read_atl10` function and put it in a module.  The function is imported into this notebook.  If your are interested, take a look at `read_atl10` in `readers.py`.  The main features of the function are briefly described here.\n",
    "\n",
    "We follow the processing steps for ATL20 to generate our freeboard grids.  For each grid cell that contain one or more freeboard segments, a grid cell mean freeboard is calculated as a mean of `gtx/freeboard_segment/beam_fb_height` from ATL10, weighted by segment length `gtx/freeboard_segment/heights/height_segment_length_seg`.  To resample segments to grid cells, we also need the geodetic coordinates for each segment in `gtx/freeboard_segment/latitude` and `gtx/freeboard_segment/longitude`. As an additional locator, we also read `gtx/freeboard_segment/delta_time`.  `gtx` is the beam number.\n",
    "\n",
    "In addition to the segment data, we also need some ancillary data from each file.  In ATL20 gridded freeboards are calculated using only the _strong beams_ of each beam pair.  Which of the six beams are strong and which are weak depends on the orientation of the ICESat-1 satellite.  Satellite orientation is given in the `orbit_info/sc_orient` dataset.  We also need to read the Atlas Standard Data Product Epoch that is stored in `ancillary_data/atlas_sdp_gps_epoch` to convert `delta_time` from seconds since launch to date and time.\n",
    "\n",
    "```{note}\n",
    "There are three beam pairs numbered 1, 2 and 3.  Each of these beam pairs has a left and right beam.  Beams are numbered `gt1l` and `gt1r`, `gt2l` and `gt2r`, and `gt3l` and `gt3r`.  Depending on the orientation of the ICESat-2 satellite, left beams or right beams are the _strong beams_.    The orientation can be _forward_ or _backward_, or _transition_.  We only use data in forward or backward orientations.\n",
    "```\n",
    "\n",
    "The datasets containing segment data are stored in the `DATASETS` constant, which is a python `list`, in `reader.py`.  If you want additional or different datasets, you can modify this list.\n",
    "\n",
    "A ATL10 file is read using the function `read_atl10`.  This function encapsulates opening an HDF5 file and reading the datasets using `h5coro`, and then creating a `geopandas.DataFrame` containing the data.  We parallelize the reading of all files in a year using `dask`, so files are read using different processors.  File for a given year are then concatenated into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc14401-66a6-44ba-b8f2-421f45e50c29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tracks = read_atl10_local(file_tree[\"2019\"], executors=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f577fc-7c75-456e-b7bb-2a4f45ab77c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f17a22-5578-4dfb-a474-218741ac8a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tracks.info(memory_usage='deep')  # what does this do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27844c4b-bdb2-431b-9c85-8afd5ed09f0d",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### For future IO eficient operations we save the geodataframe as parquet\n",
    "\n",
    "_Not sure we need this for the demo_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2369070-532f-4b0a-a1d8-0e424d0701af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tracks[\"delta_time\"] = tracks[\"delta_time\"].astype('datetime64[s]')\n",
    "# tracks.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47292a7-25e7-43a5-8c45-9fecfdb21c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tracks.to_parquet(\"atl10-2019.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63674b7-c92a-4bc1-818c-9dae0cf9cc69",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Grid the track data\n",
    "\n",
    "The resampling and calculation of statistics follows the processing steps described in the ATL20 - Gridded Sea Ice Freeboard - ATBD but gridding to a EASE-Grid v2 6.25 km grid.  Any projected coordinate system or grid could be chosen.  The procedure could be modified with extra QC steps or modifications.  **The world is your oyster - or [Aplacophoran](https://antarcticsun.usap.gov/science/4447/)**.\n",
    "\n",
    "The processing steps are:\n",
    "\n",
    "- remove non-ice and low quality segments \n",
    "- resample freeboard segments to a grid\n",
    "- calculate aggregate statistics\n",
    "    + mean segment length\n",
    "    + segment count\n",
    "    + length weighted mean freeboard\n",
    "    + length weighted standard deviation of freeboard\n",
    "    \n",
    "### Resample Freeboard Segments to a Grid\n",
    "\n",
    "Following the ATL20 ATBD, we will use a _drop-in-the-bucket_ resampling scheme.  This is simple and relatively easy to implement.  More complex resampling schemes could be substituted.\n",
    "\n",
    "To demonstrate resampling we will resample freeboard segments to WGS84 / NSIDC EASE-Grid v2.0 South with a grid resolution of 6.25 km.  The EPSG code for the WGS84 / NSIDC EASE-Grid South coordinate reference system is [6932](https://epsg.org/crs_6932/WGS-84-NSIDC-EASE-Grid-2-0-South.html).\n",
    "\n",
    "We will use the standard 6.25 km grid.  To define the grid, we need the grid dimensions (nrows and ncols), the x and y projected coordinates of the upper-left corner of the upper-left grid cell, and the height and width of the grid cells in the same units as the projected coordinates.  In this case, the units are meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427548f4-9adb-4cee-adc1-b721bddcb7a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "easegrid2_epsg = 6932\n",
    "\n",
    "# nrow = 2880\n",
    "# ncol = 2880\n",
    "# upper_left_x = -9000000.0\n",
    "# upper_left_y = 9000000.0\n",
    "# width = 6250.0\n",
    "# height = -6250.0\n",
    "\n",
    "nrow = 151\n",
    "ncol = 147\n",
    "width = 10000.0\n",
    "height = -10000.0\n",
    "upper_left_x = -1040000.0\n",
    "upper_left_y = -560000.0\n",
    "\n",
    "map_extent = [upper_left_x, (upper_left_x + (ncol*width)), (upper_left_y + (nrow*height)), upper_left_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed0d70b-253b-4ced-a354-6c7a20637640",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The first step is to reproject the points from geodetic coordinates (latitude and longitude) to projected coordinates (x, y).  Because the data are in a `geopandas.DataFrame` we can use the `to_crs` method.  This takes an EPSG code either as a numeric value (`6932`) or as a string (`\"EPSG:6932\"`).\n",
    "\n",
    "You can see that the `POINT` objects in the `geometry` have changed from having latitudes and longitudes as coordinates to x and y in meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78293c52-ad08-45a1-bd66-99b2eaade3e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tracks = tracks.to_crs(easegrid2_epsg)\n",
    "tracks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ea27df-9bea-409f-a754-d945fb7ae01a",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "A _Drop-in-the-Bucket_ resampling scheme collects points into the grid cells that they intersect with, and then calculates aggregate statistics for each grid cell using attributes associated with those points.\n",
    "\n",
    "We'll find the grid cell that contains each segment by calculating the row and column coordinates for each segment from the projected coordinates.  This is done by creating an _Affine_ transformation matrix for the grid.  The Affine matrix is just a matrix representation of the algebraic expressions to convert row and column indices of the grid to projected coordinates.  The equations below give the forward transformation from `(row, col)` to `(x, y)`. \n",
    "\n",
    "$$\n",
    "x = width * col + upper\\_left\\_x \\\\\n",
    "y = height * row + upper\\_left\\_y\n",
    "$$\n",
    "\n",
    "These are expressed in matrix form:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "0\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "a & 0 & c \\\\\n",
    "0 & d & e \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "col \\\\\n",
    "row \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $a$ is $\\mathsf{width}$, $c$ is $\\mathsf{upper\\_left\\_x}$, $d$ is $height$, and $e$ is $upper\\_left\\_y$.\n",
    "\n",
    "```{note}\n",
    "The projected coordinate system we are using is a cartesian plane with the origin at the South Pole.  The `x` coordinates increase to the right, and `y` coordinates increase up.  For raster data, which includes grids and images, have the origin at the upper-left corner of the grid.  Column indices increase from right to left, and row indices increase from top to bottom.\n",
    "```\n",
    "\n",
    "We use the `affine` package to create a forward transformation matrix (`fwd`) using the grid parameters above.  To transform `(x, y)` projected coordinates to `(row, col)`, we can calculate the reverse transformation matrix using `~fwd`.\n",
    "\n",
    "`(row, col)` coordinates are still rational numbers.  We want an integer row and column indices for grid cells.  We can use the `floor` function to get integers.  `row` and `column` indices are zero based.\n",
    "\n",
    "We want to be able to leverage the `geopandas.Dataframe.groupby` functionality to collect points into grid cells, so we need a unique identifier to group the data.  We can calculate a unique cell index from `row` and `column` indices as follows:\n",
    "\n",
    "$$\n",
    "cell\\_index = row * ncol + col\n",
    "$$\n",
    "\n",
    "This is encapsulated in the function `get_grid_index`.  This function is then applied to the `geometry` of tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00314673-7229-4be9-8674-0f39c9f29baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_grid_index(xy):\n",
    "    geotransform = (upper_left_x, width, 0., upper_left_y, 0., height)\n",
    "    fwd = Affine.from_gdal(*geotransform)\n",
    "    col, row = ~fwd * xy\n",
    "    return (np.floor(row) * ncol) + np.floor(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde50514-c70a-499c-ae77-ffadf367c6df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tracks[\"grid_index\"] = [get_grid_index((x, y)) for x, y in zip(tracks.geometry.x, tracks.geometry.y)]\n",
    "tracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0120bb43-3686-4da8-ac06-76b7f0b1b194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tracks.grid_index.min(), tracks.grid_index.max(), nrow*ncol - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8ef611-f970-4615-9e18-df848a103dd6",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Calculate grid cell mean statistics\n",
    "\n",
    "We calculate four statistics for grid cells that contain segments.\n",
    "\n",
    "#### Grid Cell Mean Segment Length $\\bar{L}$\n",
    "\n",
    "$$\n",
    "\\bar{L}(x, y, D) = \\frac{\\sum L_i}{N}\n",
    "$$\n",
    "\n",
    "where $L_i$ is `/gtx/freeboard_beam_segment/height_segments/height_segment_length_seg`, $x$ and $y$ are projected coordinates for grid centers, and $D$ is day. \n",
    "\n",
    "#### Grid Cell Mean Freeboard $\\bar{h}$\n",
    "\n",
    "$$\n",
    "\\bar{h}(x, y, D) = \\frac{\\sum L_i h_i}{\\sum L_i}\n",
    "$$\n",
    "\n",
    "where $h_i$ is `gtx/freeboard_beam_segment/beam_freeboard/beam_fb_height`.\n",
    "\n",
    "#### Grid Cell Standard Deviation of Freeboard $\\sigma^2 (x, y, D)$\n",
    "\n",
    "$$\n",
    "\\sigma^2 (x, y, D) = \\frac{\\sum L_i (h_i)^2}{\\sum L_i} - \\bar{h}^2 (x, y, D)\n",
    "$$\n",
    "\n",
    "The functions to calculate these statistics are given below.  These functions are applied to the grouped data.  The `geopandas.apply` method only accepts a single method when operating on multiple columns in a dataframe.  We could just have multiple calls for each aggregating function.  However, we can collect the individual aggregating functions into a single function and pass that to the `apply` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b0e9c9-b81e-4e71-b29a-7b04b6c42b15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_segment_length(df):\n",
    "    \"\"\"Returns mean segment length\"\"\"\n",
    "    return df[\"height_segment_length_seg\"].mean()\n",
    "\n",
    "\n",
    "def mean_freeboard(df):\n",
    "    \"\"\"Returns length weighted mean freeboard\"\"\"\n",
    "    return (df.beam_fb_height * df.height_segment_length_seg).sum() / df.height_segment_length_seg.sum()\n",
    "\n",
    "\n",
    "def stdev_freeboard(df):\n",
    "    \"\"\"Returns weighted standard deviation of freeboard\"\"\"\n",
    "    hmean = mean_freeboard(df)\n",
    "    stdev = (df.beam_fb_height**2 * df.height_segment_length_seg).sum() / df.height_segment_length_seg.sum()\n",
    "    return stdev - hmean**2\n",
    "\n",
    "\n",
    "def count_segments(df):\n",
    "    \"\"\"Number of segments in grid cell\"\"\"\n",
    "    return df.beam_fb_height.count()\n",
    "\n",
    "\n",
    "def all_funcs(x):\n",
    "    \"\"\"Wrapper that allows all the aggregation functions to be applied at once\"\"\"\n",
    "    funcs = {\n",
    "        mean_segment_length.__name__: mean_segment_length(x),  #__name__ gets the name of a function\n",
    "        mean_freeboard.__name__: mean_freeboard(x),\n",
    "        stdev_freeboard.__name__: stdev_freeboard(x),\n",
    "        count_segments.__name__: count_segments(x),\n",
    "    }\n",
    "    # `apply` is expected to return a series or a scaler so we collect the results\n",
    "    # into a series indexed by aggregating function name\n",
    "    return pd.Series(funcs, index=funcs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f5be24-a611-4f05-ad99-10d07d1fa7ef",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Testing the functions\n",
    "\n",
    "It is always a good idea to test your code.  Below are some test data and expected results.  The functions are tested on `test_df`.  We then use `pandas.testing.assert_frame_equal` to check that the result and expected dataframes are the same.  In this case we are only interested getting the same values, so we do not check the names or datatypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888fdcb-f08a-4164-b740-f25d25a92aba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(\n",
    "    {\n",
    "        'grid_index': [1, 1, 1, 2, 2, 2, 2],\n",
    "        \"height_segment_length_seg\": [1.2, 1.1, 0.7, 2.3, 1.5, .9, 1.],\n",
    "        \"beam_fb_height\": [0., 0.2, 0.5, 1.1, 2., .9, 1.5],        \n",
    "    }\n",
    ")\n",
    "expected = pd.DataFrame(\n",
    "    {\n",
    "        \"mean_segment_length\": [1.0, 1.425],\n",
    "        \"mean_freeboard\": [0.19000000000000003, 1.375438596491228],\n",
    "        \"stdev_freeboard\": [0.03689999999999998, 0.17167743921206569],\n",
    "        \"count_segments\": [3, 4],\n",
    "    },\n",
    "    index = [1, 2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9696fc-7d63-4d5b-9d41-a95e5c408875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = test_df.groupby(\"grid_index\").apply(all_funcs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7fe215-e265-4c6b-ae21-f2ca1dfbdee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.testing.assert_frame_equal(expected, result, check_names=False, check_dtype=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6e82a2-569d-46c8-b7ba-53bd42774ac7",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Now that we have functions that work we can apply them to the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430c3cf-a667-43df-8313-701fdfc1abf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "aggregated_data = tracks.groupby(\"grid_index\").apply(all_funcs)\n",
    "aggregated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d50d44e-3cb1-4d82-9711-619258d4308b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Assign aggregated data to grid cells\n",
    "\n",
    "We now have a dataframe that contains grid cell statistics indexed by a unique array index.  We can now create a grid for each of these statistics.\n",
    "\n",
    "The procedure is relatively straight forward.\n",
    "\n",
    " - Create an 1D array with the same number of elements as cells in our grid.\n",
    " - Use the `grid_index` of the dataframe as an array index to assign values to grid cells, where we have data.\n",
    " - Reshape the grid to the dimension of the grid.\n",
    " \n",
    "We can encapsulate this in a `series_to_grid` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f1a761-2fa1-4700-8dc2-481b419ee2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def series_to_grid(series, nrow, ncol):\n",
    "    \"\"\"Converts a geopandas.Series to a grid using the index\"\"\"\n",
    "    these_points = series.index < (nrow*ncol - 1)\n",
    "    \n",
    "    array_index = series[these_points].index.values.astype(int)  # the array index must be an integer\n",
    "    \n",
    "    vector = np.full(nrow*ncol, np.nan)\n",
    "    vector[array_index] = series[these_points]\n",
    "    return vector.reshape(nrow, ncol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf83d5-b888-427d-b4f0-e98beae7845f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grids = {name: series_to_grid(values, nrow, ncol) for name, values in aggregated_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6994fca1-53aa-4b76-95d9-c24981bb4100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.imshow(grids['count_segments'], interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b73ba-e759-43cb-ad11-4b24c79eb75b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Plot data on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8454d0e8-fd29-45f6-b5aa-f15947ea95de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#proj = EASEGrid2South()\n",
    "plt.close(\"all\")\n",
    "proj = ccrs.LambertAzimuthalEqualArea(central_latitude=-90)\n",
    "\n",
    "test_extent = [-3000000.0, 3000000.0, -3000000.0, 3000000.0]\n",
    "#np.array(map_extent)+np.array([-1e6,-1e6,1e6,1e6])\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection=proj)\n",
    "ax.set_extent(map_extent, proj)\n",
    "ax.add_feature(cfeature.LAND)\n",
    "ax.coastlines()\n",
    "\n",
    "plt.imshow(grids['count_segments'], interpolation='none', extent=map_extent)\n",
    "#ax.set_extent("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b30ee90-e32c-4abe-9dc4-729fb4ab8b30",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Appendix\n",
    "\n",
    "### Get grid parameters for Ross Sea region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc7abd-05bc-4c2b-ba17-e5f375707bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import GeoJSON of Ross Sea - this is very approximate\n",
    "import geopandas as gpd\n",
    "\n",
    "ross_sea_gdf = gpd.read_file(\"ross_sea.json\")\n",
    "bounds = ross_sea_gdf.to_crs(easegrid2_epsg).bounds.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238b9cb-d2d8-4157-90b0-7c8a429dfdbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate parameters for a grid with resolution that covers region\n",
    "resolution = 10000.\n",
    "minx, miny, maxx, maxy = [func(bound/resolution) * resolution for bound, func in zip(list(bounds), [np.floor, np.floor, np.ceil, np.ceil])][0]\n",
    "\n",
    "grid_extent_x = maxx - minx\n",
    "grid_extent_y = maxy - miny\n",
    "\n",
    "width = height = resolution\n",
    "\n",
    "ncol = grid_extent_x / width\n",
    "nrow = grid_extent_y / height\n",
    "\n",
    "upper_left_x = minx\n",
    "upper_left_y = maxy\n",
    "\n",
    "print(f\"nrow = {int(nrow)}\")\n",
    "print(f\"ncol = {int(ncol)}\")\n",
    "print(f\"width = {width}\")\n",
    "print(f\"height = -{height}\")\n",
    "print(f\"upper_left_x = {upper_left_x}\")\n",
    "print(f\"upper_left_y = {upper_left_y}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b640585b-9853-40dc-8aeb-e190bb4a24b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from cartopy.crs import AzimuthalEquidistant\n",
    "\n",
    "# class EASEGrid2South(AzimuthalEquidistant):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(EASEGrid2South, self).__init__(central_longitude=0.0, central_latitude=-90.0,\n",
    "#                  false_easting=0.0, false_northing=0.0,\n",
    "#                  globe=None)\n",
    "        \n",
    "#         self._bounds = [-9000000.0, -9000000.0, 9000000.0, 9000000.0]\n",
    "#         self._x_limits = self._bounds[0], self._bounds[2]\n",
    "#         self._y_limits = self._bounds[1], self._bounds[3]\n",
    "        \n",
    "\n",
    "#     @property\n",
    "#     def bounds(self):\n",
    "#         return self._bounds\n",
    "    \n",
    "#     @property\n",
    "#     def threshold(self):\n",
    "#         return 1e5\n",
    "\n",
    "#     @property\n",
    "#     def x_limits(self):\n",
    "#         return self._x_limits\n",
    "\n",
    "#     @property\n",
    "#     def y_limits(self):\n",
    "#         return self._y_limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f068986-0428-4fa2-855a-c179016a6f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e92549f7-1bb0-4373-b810-1813d450fb31",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Assign to grid and calculate grid cell mean**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b2ef6-3e14-475e-b689-77bda4c1814e",
   "metadata": {},
   "source": [
    "## **3. Learning outcomes recap (optional)**\n",
    "\n",
    "Provide a brief summary of the learning outcomes of the tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87da360-fa68-43e6-993e-5d5098b70aed",
   "metadata": {},
   "source": [
    "## **4. Additional resources (optional)**\n",
    "\n",
    "List some additional resources for users to consult, if applicable/desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178b3e1e-71e4-4aab-bdbe-4d33dbce01e9",
   "metadata": {},
   "source": [
    "________\n",
    "\n",
    "### **When your tutorial is ready for review,  please read our [Contributor Guide](https://github.com/nsidc/NSIDC-Data-Tutorials/blob/main/contributor_guide.md) for next steps.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7019c-b0c8-4c96-a36c-9cb1d17b6303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
