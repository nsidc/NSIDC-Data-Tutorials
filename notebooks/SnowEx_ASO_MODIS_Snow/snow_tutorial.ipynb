{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snow Depth and Snow Cover Data Exploration \n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to access and compare coincident snow data from in-situ Ground Pentrating Radar (GPR) measurements, and airborne and satellite platforms from NASA's SnowEx, ASO, and MODIS data sets. All data are available from the NASA National Snow and Ice Data Center Distributed Active Archive Center (NSIDC DAAC). \n",
    "\n",
    "## What you will learn in this tutorial\n",
    "\n",
    "In this tutorial you will learn:\n",
    "\n",
    "1. what snow data and information is available from NSIDC and the resources available to search and access this data;\n",
    "2. how to search and access spatiotemporally coincident data across in-situ, airborne, and satellite observations.\n",
    "3. how to read SnowEx GPR data into a Geopandas GeoDataFrame;\n",
    "4. how to read ASO snow depth data from GeoTIFF files using xarray;\n",
    "5. how to read MODIS Snow Cover data from HDF-EOS files using xarray;\n",
    "6. how to subset gridded data using a bounding box;\n",
    "5. how to extract and visualize raster values at point locations;\n",
    "6. how to save output as shapefile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snow data and resources at NSIDC DAAC\n",
    "<!-- \n",
    "[The National Snow and Ice Data Center](https://nsidc.org) provides over 1100 data sets covering the Earth's cryosphere and more, all of which are available to the public free of charge. NSIDC creates supports data users, creates tools for data access, performs scientific research, and educates the public about the cryosphere. \n",
    "\n",
    "#### Selected NSIDC DAAC and NASA Data Resources\n",
    "\n",
    "* [NSIDC Data Search](https://nsidc.org/data/search/#keywords=snow)\n",
    "    * Search NSIDC snow data\n",
    "* [NSIDC Data Update Announcements](https://nsidc.org/the-drift/data-update/) \n",
    "    * News and tips for data users\n",
    "* [NASA Earthdata Search](http://search.earthdata.nasa.gov/)\n",
    "    * Search and access data across the NASA Earthdata\n",
    "* [NASA Worldview](https://worldview.earthdata.nasa.gov/)\n",
    "    * Interactive interface for browsing full-resolution, global, daily satellite images\n",
    "    \n",
    "    \n",
    "#### Snow Today\n",
    "\n",
    "[Snow Today](https://nsidc.org/snow-today), a collaboration with the University of Colorado's Institute of Alpine and Arctic Research (INSTAAR), provides near-real-time snow analysis for the western United States and regular reports on conditions during the winter season. Snow Today is funded by NASA Hydrological Sciences Program and utilizes data from the Moderate Resolution Imaging Spectroradiometer (MODIS) instrument and snow station data from the Snow Telemetry (SNOTEL) network by the Natural Resources Conservation Service (NRCS), United States Department of Agriculture (USDA) and the California Department of Water Resources: www.wcc.nrcs.usda.gov/snow.\n",
    "\n",
    "### Snow-related missions and data sets featured in this tutorial: -->\n",
    "\n",
    "In this tutorial we use snow depth and snow cover data collected on the Grand Mesa, Colorado, during NASA's SnowEx 2017 campaign.  [SnowEx]() was a multi-year field experiment to collect an extensive set of measurements of snow cover characteristics and conditions, in conjunction with airborne and satellite data, to assess the ability of different remote sensing techniques to measure snow pack characteristics in a variety of snow environments.\n",
    "\n",
    "We use snow depths estimated from surface-based ground penetrating radar (GPR) and the Airborne Snow Observatory (ASO) airborne lidar, and fractional snow cover area retrieved from the MODIS/Terra satellite.  The links to the dataset landing pages are below.\n",
    "\n",
    "| Dataset | Short Name | Version | DOI |\n",
    "|---------|------------|---------|------------------|\n",
    "| SnowEx17 Ground Penetrating Radar | SNEX17_GPR | 2 | https://doi.org/10.5067/G21LGCNLFSC5 |\n",
    "| ASO L4 Lidar Snow Depth 3m UTM Grid | ASO_3M_SD | 1 | https://doi.org/10.5067/KIE9QNVG7HP0 |\n",
    "| MODIS/Terra Snow Cover Daily L3 Global 500m SIN Grid | MOD10A1 | 6 | https://doi.org/10.5067/MODIS/MOD10A1.006 |\n",
    "\n",
    "<!-- * [SnowEx](https://nsidc.org/data/snowex)\n",
    "    * SnowEx17 Ground Penetrating Radar, Version 2: https://doi.org/10.5067/G21LGCNLFSC5\n",
    "* [ASO](https://nsidc.org/data/aso)\n",
    "    * ASO L4 Lidar Snow Depth 3m UTM Grid, Version 1: https://doi.org/10.5067/KIE9QNVG7HP0\n",
    "* [MODIS](https://nsidc.org/data/modis)\n",
    "    * MODIS/Terra Snow Cover Daily L3 Global 500m SIN Grid, Version 6: https://doi.org/10.5067/MODIS/MOD10A1.006\n",
    " -->\n",
    "<!-- data_dict = { 'snowex': {'short_name': '','version': '2','polygon': polygon,'temporal':temporal},\n",
    "                'aso': {'short_name': 'ASO_3M_SD','version': '1','polygon': polygon,'temporal':temporal},\n",
    "             'modis': {'short_name': 'MOD10A1','version': '6','polygon': polygon,'temporal':temporal}\n",
    "            }\n",
    " -->\n",
    " <!-- \n",
    "#### Other relevant snow products:\n",
    "\n",
    "In addition to the three datasets featured in this tutorial, NSIDC hosts many other snow datasets.  A selection is listed below.\n",
    "\n",
    "* [VIIRS Snow Data](http://nsidc.org/data/search/#sortKeys=score,,desc/facetFilters=%257B%2522facet_sensor%2522%253A%255B%2522Visible-Infrared%2520Imager-Radiometer%2520Suite%2520%257C%2520VIIRS%2522%255D%252C%2522facet_parameter%2522%253A%255B%2522SNOW%2520COVER%2522%252C%2522Snow%2520Cover%2522%255D%257D/pageNumber=1/itemsPerPage=25)\n",
    "\n",
    "* [AMSR-E and AMSR-E/AMSR2 Unified Snow Data](http://nsidc.org/data/search/#sortKeys=score,,desc/facetFilters=%257B%2522facet_sensor%2522%253A%255B%2522Advanced%2520Microwave%2520Scanning%2520Radiometer-EOS%2520%257C%2520AMSR-E%2522%252C%2522Advanced%2520Microwave%2520Scanning%2520Radiometer%25202%2520%257C%2520AMSR2%2522%255D%252C%2522facet_parameter%2522%253A%255B%2522SNOW%2520WATER%2520EQUIVALENT%2522%252C%2522Snow%2520Water%2520Equivalent%2522%255D%257D/pageNumber=1/itemsPerPage=25)\n",
    "\n",
    "* [MEaSUREs Snow Data](http://nsidc.org/data/search/#keywords=measures/sortKeys=score,,desc/facetFilters=%257B%2522facet_parameter%2522%253A%255B%2522SNOW%2520COVER%2522%255D%252C%2522facet_sponsored_program%2522%253A%255B%2522NASA%2520National%2520Snow%2520and%2520Ice%2520Data%2520Center%2520Distributed%2520Active%2520Archive%2520Center%2520%257C%2520NASA%2520NSIDC%2520DAAC%2522%255D%252C%2522facet_format%2522%253A%255B%2522NetCDF%2522%255D%252C%2522facet_temporal_duration%2522%253A%255B%252210%252B%2520years%2522%255D%257D/pageNumber=1/itemsPerPage=25)\n",
    "  \n",
    "* [Near-Real-Time SSM/I-SSMIS EASE-Grid Daily Global Ice Concentration and Snow Extent (NISE), Version 5](https://doi.org/10.5067/3KB2JPLFPK3R)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "\n",
    "We will start by importing the packages we use in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For notebook rendering\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# Adding this to suppress runtime and deprecations warnings  \n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# For search and access\n",
    "import earthaccess\n",
    "\n",
    "# For reading SnowEx GPR data\n",
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point, box #, mapping\n",
    "\n",
    "# For reading ASO and MODIS\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "\n",
    "# For Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import Normalize\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# Miscellaneous imports\n",
    "import dateutil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Discovery\n",
    "\n",
    "We start by identifying the study area and time-range using the spatial and temporal coverage of the SnowEx GPR surveys and then searching for ASO and MODIS data collected for the same time and area. \n",
    "\n",
    "<!-- NASA Earthdata Search can be used to visualize the coverage over mulitple data sets and to access the same data you will be working with below: \n",
    "https://search.earthdata.nasa.gov/projects?projectId=5366449248\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get study area and time-range for SnowEx GPR\n",
    "\n",
    "The NASA SnowEx 2017 field experiment was conducted on the Grand Mesa, Colorado.  Observations were collected between September 2016 and July 2017, with an intensive observing period from 6 February to 25 February, 2017.  \n",
    "\n",
    "There are a number of ways to get the spatial coverage of this dataset.\n",
    "\n",
    "1. Use the Spatial Coverage of the dataset from the [Overview](https://nsidc.org/data/snex17_gpr/versions/2#anchor-overview) section of the dataset landing page.\n",
    "2. Draw a polygon for your area of interest on the map in the [Data Access Tool](https://nsidc.org/data/data-access-tool/SNEX17_GPR/versions/2) for the data.\n",
    "3. Retrieve the bounding polygon from the collection metadata using the `earthaccess` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1. Use Spatial Coverage from dataset landing page\n",
    "\n",
    "The Overview section of the SnowEx17 GPR dataset landing page gives the **Spatial Coverage** of the data collection.\n",
    "\n",
    "<img align=\"left\" src=\"images/data_landing_page_overview.png\">\n",
    "\n",
    "We can see that the latitude and longitude ranges for the collection are:\n",
    "- 39.11115 N to 38.9935 N  \n",
    "- -108.22367 E to -107.85785 E  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define create a bounding box that can be passed to `earthaccess`, we simply copy these values into a Python tuple in the order \n",
    "\n",
    "```\n",
    "(lower_left_longitude, lower_left_latitude, upper_right_longitude, upper_right_latitude)\n",
    "```\n",
    "\n",
    "For the values above this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_bbox = (-108.22367, 39.11115, -107.85785, 38.9935)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2. Draw and export a region of interest using the Data Access Tool map\n",
    "\n",
    "NSIDC's Data Access Tool allows you to draw and export a polygon to define your region of interest.  To go to the Data Access Tool, click on \"Data Access and Tools\" in the menu on the right side of the dataset landing page.  Then select the \"Data Access Tool\" card by clicking \"Data Access Tool\".\n",
    "\n",
    "<img align=\"left\" src=\"images/data_download_polygon_export.png\">\n",
    "\n",
    "Click on the Polygon Drawing button and create a polygon by clicking on the map to add points.  Finish drawing the polygon by clicking on the first point you added.  The shape of the polygon can be edited by dragging the points.\n",
    "\n",
    "To export the polygon, click on the \"Floppy Disk\" icon.  The polygon is exported as a GeoJSON file.  An example is shown below.\n",
    "\n",
    "```\n",
    "{\n",
    "  \"type\": \"Feature\",\n",
    "  \"geometry\": {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "      [\n",
    "        [\n",
    "          -108.2352445938561,\n",
    "          38.98556907427165\n",
    "        ],\n",
    "        [\n",
    "          -107.85284607930835,\n",
    "          38.978765032966244\n",
    "        ],\n",
    "        [\n",
    "          -107.85494925720668,\n",
    "          39.10596902171742\n",
    "        ],\n",
    "        [\n",
    "          -108.22772795408136,\n",
    "          39.11294532581687\n",
    "        ],\n",
    "        [\n",
    "          -108.2352445938561,\n",
    "          38.98556907427165\n",
    "        ]\n",
    "      ]\n",
    "    ]\n",
    "  },\n",
    "  \"properties\": {}\n",
    "}\n",
    "```\n",
    "\n",
    "An example polygon geojson file is provided in the /Data folder of this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Geopandas to read the GeoJSON file.  This returns a Geopandas GeoSeries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_polygon_gdf = gpd.read_file('Data/nsidc-polygon.json')\n",
    "roi_polygon_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a polygon for `earthaccess`, we create a list of tuples from the GeoSeries.\n",
    "\n",
    "_check that earthaccess checks orientation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_polygon = [tuple(xy.values) for _, xy in roi_polygon_gdf.get_coordinates().iterrows()]\n",
    "roi_polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 3. Retrieve Spatial Coverage from metdata using `earthaccess`\n",
    "\n",
    "`earthaccess.search_datasets` returns a list of objects containing metadata for datasets found.  This metadata contains the spatial extent of the dataset.\n",
    "\n",
    "We search for the SnowEx17 GPR dataset using `earthaccess`.  This has the shortname \"SNEX17_GPR\".  We want version 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = earthaccess.search_datasets(\n",
    "    short_name = \"SNEX17_GPR\",\n",
    "    version = '2',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a single dataset as a Python list with length 1.  The metadata object contained in the list is a mixture of nested Python dictionaries and lists.  You can inspect the structure by typing `print(r[0])`.\n",
    "\n",
    "For the SnowEx17 GPR dataset, spatial extent is described as a bounding box.  It can be found at:\n",
    "\n",
    "```\n",
    "umm/SpatialExtent/HorizontalSpatialDomain/Geometry/BoundingRectangles\n",
    "```\n",
    "\n",
    "We translate this path into the keys of a nested Python dictionary, as we do in the code cell below.  The value of `BoundingRectangles` is a list because there can be more than one bounding rectangle.  However, in this case, we know there is only one bounding rectangle, so we get the first element of that list.  Also note that we have to get the first element of the results `r` from `search_datasets`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_coverage = r[0]['umm']['SpatialExtent']['HorizontalSpatialDomain']['Geometry']['BoundingRectangles'][0]\n",
    "spatial_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BoundingRectangle` is returned as a dictionary.  We have to transform this into a tuple `(xmin, ymin, xmax, ymax)` that is expected by `earthaccess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_bbox = (\n",
    "    spatial_coverage['WestBoundingCoordinate'],\n",
    "    spatial_coverage['SouthBoundingCoordinate'],\n",
    "    spatial_coverage['EastBoundingCoordinate'],\n",
    "    spatial_coverage['NorthBoundingCoordinate']\n",
    ")\n",
    "roi_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for Data\n",
    "\n",
    "Now that we have a bounding box saved as `roi_bbox` for the SnowEx17 GPR we can use it to look for ASO and MODIS data.  First, we will see what GPR data are available.  We do this using `earthaccess.search_data`.  This is similar to `earthaccess.search_datasets` but looks for data files (also called granules) instead of datasets.\n",
    "\n",
    "We could use our region of interest bounding box or polygons but we don't need these for the SnowEx17 GPR data because we know this data is in pretty much the same location.  So we just supply the dataset short name and version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowex_result = earthaccess.search_data(\n",
    "    short_name = \"SNEX17_GPR\",\n",
    "    version = '2',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three files found.  We can get some basic information about these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[display(result) for result in snowex_result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to refine our search for coincident ASO and MODIS data, we need the beginning and end time and date of each GPR survey.  This is contained in the file metadata and we can access this in a similar way to how we got the spatial extent for the SnowEx data collection.\n",
    "\n",
    "Below, we get the file name, beginning date and time, and ending date and time for each SnowEx17 GPR file found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in snowex_result:\n",
    "    print(\n",
    "        f\"Granule-ID: {r['umm']['GranuleUR']}\\n\",\n",
    "        f\"     Begin: {r['umm']['TemporalExtent']['RangeDateTime']['BeginningDateTime']}\\n\"\n",
    "        f\"       End: {r['umm']['TemporalExtent']['RangeDateTime']['EndingDateTime']}\\n\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of the tutorial, we are going to focus on the GPR survey collected in week 1 and compare snow depths retrieved from this survey with snow depth from ASO and snow cover fraction from MODIS.\n",
    "\n",
    "We'll set a temporal range for the ASO and MODIS data searches using the beginning and ending datetimes for the week 1 survey.  We could do this by copying the dates by hand but this means that if you want to change the date range of the search you have to find the cell with the dates and manually change them.  It is better to automate the process.  This also avoids cut-and-paste mistakes. \n",
    "\n",
    "To facilitate this, we will create a `survey_id` variable with a value `0`.  Then if we want to use a different survey, we can just change the `survey_id` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_id = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the beginning and ending datetimes for the first survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_datetime = snowex_result[survey_id]['umm']['TemporalExtent']['RangeDateTime']['BeginningDateTime']\n",
    "end_datetime = snowex_result[survey_id]['umm']['TemporalExtent']['RangeDateTime']['EndingDateTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a `temporal_range` that we can use in searches for ASO and MODIS.  \n",
    "\n",
    "We'll parse the `begin_datetime` and `end_datetime` into `datetime` objects using the `dateutil` package.  This avoids inputting incorrect formats to the `earthaccess` and CMR search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_range = (\n",
    "    dateutil.parser.isoparse(begin_datetime), \n",
    "    dateutil.parser.isoparse(end_datetime)\n",
    ")\n",
    "temporal_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two datetime objects represent the date range.  They are `datetime` objects.  To see the times in ISO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Temporal Range: {temporal_range[0].isoformat()} to {temporal_range[1].isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for ASO Flightlines\n",
    "\n",
    "Now that we have a region of interest and a date range defined, we can search for coincident ASO and MODIS data. \n",
    "\n",
    "From the table of datasets we know that the `short_name` for the ASO data is `ASO_3M_SD` and we want version 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aso_result = earthaccess.search_data(\n",
    "    short_name = \"ASO_3M_SD\",\n",
    "    version = '1',\n",
    "    bounding_box = roi_bbox,\n",
    "    temporal = temporal_range,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns one granule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(aso_result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for MODIS Snow Cover Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_result = earthaccess.search_data(\n",
    "    short_name = \"MOD10A1\",\n",
    "    version = \"61\",\n",
    "    bounding_box = roi_bbox,\n",
    "    temporal = temporal_range,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three MODIS scenes.  We can use display again to see an overview of the results.  You can click on the thumbnails to download a larger version.  The green region is snow free land, the blue is cloud cover and the orange hues are snow cover fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[display(r) for r in modis_result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access and Read the Data\n",
    "\n",
    "In this section we are going to access the data granules and read granules into data objects for visualization and analysis.  A data object, in this context, is a Python data structure that contains the data values and associated metadata, and has a set of methods associated with it.  \n",
    "\n",
    "We have three datasets.  The SnowEx GPR has three surveys but we are going to use the survey from the first week.  There is one temporal and spatially coincident ASO snow depth data granule, and three MODIS scenes. From the results summaries we can see that the data is in three different file formats.  SnowEx GPR is a CSV file.  ASO snow depth is a GeoTIFF.  The MODIS snow cover data are in HDF files.  In fact this is HDF-EOS.  We will use the Pandas, Geopandas and xarray Python packages to read these data granules.\n",
    "\n",
    "All the datasets we are working with are in the cloud.  For the SnowEx ~and ASO~ datasets, rather than downloading the data, we will _stream_ the data loading it directly into memory.  Unfortunately, we cannot use this method for the MODIS snow cover data because the nested group structure of HDF-EOS does not allow this kind of access.  \n",
    "\n",
    "If you are working on a local machine and would rather download the data, use the following command, specifying the list of results returned by `earthaccess.search_data` and the local download path:\n",
    "```\n",
    "earthaccess.download(<list_of_results>, local_path=<local_download_path>)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SnowEx GPR\n",
    "\n",
    "SnowEx GPR data have the `.csv` file extension, indicating that they are comma-delimited.  This is not entirely true.  Unfortunately, files in this data collection have inconsistent formatting.  `SnowEx17_GPR_Version2_Week1.csv` and `SnowEx17_GPR_Version2_Week2.csv` are tab-delimted.  `SnowEx17_GPR_Version2_Week3.csv` is comma-delimted.\n",
    "\n",
    "We demonstrate reading week 1 but show the code below to read weeks 2 and 3.\n",
    "\n",
    "To read `SnowEx17_GPR_Version2_Week1.csv` and `SnowEx17_GPR_Version2_Week2.csv` use \n",
    "```\n",
    "pd.read_csv(<file_name>, sep='\\t')\n",
    "```\n",
    "To read `SnowEx17_GPR_Version2_Week3.csv` use\n",
    "```\n",
    "pd.read_csv(<file_name>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To _stream_ the data, we first have to open a link to the remote file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_snex = earthaccess.open(snowex_result)  # Open all the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a _list_ of _file-like objects_, that we can read using `pandas.read_csv`.  In this example, we have opened all three SnowEx granules but we only read the granule for week into a `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv(f_snex[survey_id], sep='\\t')  # f_snex[0] is week1 and tab-delimited\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for the week 1 survey were collected over multiple days between 2017-02-08 and 2017-02-10.  Because we want to find temporally coincident data, we need to subset by day.  \n",
    "\n",
    "There is no timestamp in the data but the day that data were collected is encoded in the _collection_ name column.  We will create new index containing the day of collection so that we can subset the data.\n",
    "\n",
    "We use the `re` package to perform a regular expression search and to extract the date portion of a collection name.  This date-string is then converted to a DateTime object using the `datetime` package.  This is written as the function `collection_to_date`.  We then apply this function to the _collection_ column and assign the result as the index of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime as dt\n",
    "\n",
    "def collection_to_date(x):\n",
    "    date_str = re.search(r'GPR_\\d{4}_(\\d{6})', x)\n",
    "    if date_str:\n",
    "        return dt.datetime.strptime(date_str.groups(0)[0], \"%m%d%y\")\n",
    "\n",
    "df.index = df.collection.apply(collection_to_date)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas recognizes a DataFrame with a datetime index as a time series and allows a simple subsetting based on a date string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[\"2017-02-08\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the table above, the index is the same for every row.  For future analysis, we want a unique index.  So we reset the index to a unique numeric index.  We set the name of the index first to preserve the date information for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.name = \"date\"\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting and analysis, we create a `geopandas.GeoDataFrame`.  A `GeoFataFrame` is similar to and supports all the functionality of a `pandas.DataFrame` but it has a `geometry` column and methods that allow GIS-like operations, such as spatial joins, intersections, etc.  We create the `geometry` for the `GeoDataFrame` from the latitude and longitude columns.\n",
    "\n",
    "The SnowEx data files have columns containing projected x and y coordinates.  However, there are some issues with these values.  In some files, these coordinates are for different UTM zones, which makes plotting and reprojection for the whole DataFrame difficult.  Latitude and longitude are in a consistent CRS, WGS-84 (EPSG:4326).\n",
    "\n",
    "```{note}\n",
    "The `UTM_Zone` column gives the UTM zone as `12 S`.  This is wrong and should be `12 N` for the northern hemisphere UTM zone 12.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowex_gpr = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.long, df.lat, crs=4326))\n",
    "snowex_gpr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a georeferenced set of survey points that we can plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowex_gpr.plot(\"Thickness\", legend=\"True\", legend_kwds={\"label\": \"Thickness (m)\"});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and Read ASO Snow Depth Data\n",
    "\n",
    "<!-- As with the SnowEx GPR data, we will _stream_ the ASO data and open it directly into memory.\n",
    " -->\n",
    "ASO data are GeoTIFFs.  We can use `xarray.open_dataset` with `engine=rasterio` to open a GeoTIFF.  We set `chunks='auto'` to allow _out-of-memory_ operations.  The `squeeze` method removes dimensions of length 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# f_aso = earthaccess.open(aso_result)\n",
    "f_aso = earthaccess.download(aso_result, local_path=\"download\")\n",
    "\n",
    "aso = xr.open_dataset(f_aso[0], engine='rasterio', masked=True, chunks='auto').squeeze(drop=True)\n",
    "aso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and Read MODIS Snow Cover\n",
    "\n",
    "At this time, `xarray` cannot read HDF-EOS file-like objects.  _Don't ask me why_.  So we need to download the MODIS file.  The downloaded files are written to the `download` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f_modis = earthaccess.download(modis_result, local_path='download')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF-EOS is a hierachical data format.  Data variables are organized into groups that mimic a directory structure.  To find the data we want, we need to know something about the groups in the files.  This can be found in the MOD10A1 User Guide section 1.2.2.\n",
    "\n",
    "<add figure>\n",
    "\n",
    "Looking at this figure, we can see that the data are in the \"MOD_Grid_Snow_500m\" group.\n",
    "\n",
    "Another way to discover this information is to use `gdalinfo`.  Uncomment (Ctrl/) and run the cell below.  If we scroll to the Subdataset section, there is a list of SUBDATASETs.  You can interpret these as `<filepath>:<group>:<data-variable>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning! Your gdal may not have the driver for hdf-eos\n",
    "# !gdalinfo {f_modis[0]}  # The {var} syntax is used to pass a variable in a jupyter notebook to a shell command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set `group=\"MOD_Grid_Snow_500m\"` to tell xarray to get the data in this group.  `engine=\"rasterio\"` tells `xarray` to use `rasterio`, actually GDAL, to read the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "modis = xr.open_dataset(f_modis[0], group=\"MOD_Grid_Snow_500m\", engine=\"rasterio\", chunks=\"auto\").squeeze()\n",
    "modis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an `xarray.Dataset` containing the MODIS data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Clip ASO Data to 500 m Buffer Around Mesa Lakes SNOTEL site -->\n",
    "## Clip ASO Data to the bounding-box of the SnowEx GPR data\n",
    "\n",
    "The ASO data are large.  The data can be clipped to a smaller region of interest using the `clip` method for `rioxarray.DataSets`.   As an example, we will _clip_ the ASO data from 8 February to the bounding box of the SnowEx GPR survey, using the `rioxarray` `clip` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to define the clip region.  There are several ways to do this.  Here, we use the `total_bounds` attribute for the `snowex_gpr` `GeoDataFrame`.\n",
    "\n",
    "Before we define the bounding box, we need to make sure that the ASO data and SnowEx GPR data are in the same CRS.  We use the `to_crs` method to reproject the GPR data to the CRS for ASO.  We can use the `rio` accessor to get the ASO crs\n",
    "\n",
    "```\n",
    "aso.rio.crs\n",
    "```\n",
    "\n",
    "The `rioxarray` `clip` method expects a list of geometry objects, in this case a bounding box.  We use a `shapely.geometry.box` to create a bounding box geometry object.  `box` expects for values defining _minimum-x_, _minimum-y_, _maximum-x_, and _maximum-y_.  `total_bounds` returns a tuple.  We use the `*` operator to unpack the tuple returned by `total_bounds` into four values.  The `[]` are used to create a list with one element.\n",
    "<!-- Here, we will use GeoPandas to create a `GeoSeries` containing a single point for the snotel. -->\n",
    "\n",
    "<!-- The [Mesa Lakes SNOTEL](https://wcc.sc.egov.usda.gov/nwcc/site?sitenum=622) site is located at 39.05 N and -108.05 E.  _Where did this come from?_ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_region = [box(*snowex_gpr.to_crs(aso.rio.crs).total_bounds)]  # Clip for extent of survey data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the `rioxarray` `clip` method to crop the ASO data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aso_cropped = aso.rio.clip(clip_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ASO and SnowEx GPR snow depth, and SNOTEL location\n",
    "\n",
    "We can plot the ASO Lidar snow depth and the GPR snow depth to compare the two datasets.  We plot this as a map showing the raster ASO snow depth overlaid with the GPR snow depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any comparison plot, we want to make sure that our two datasets have the same range for the color bar.  Here, we do this by getting the minimum and maximum values of the ASO data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = (aso_cropped.band_data.min().values, aso_cropped.band_data.max().values)\n",
    "vmin, vmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `matplotlib` figure and axis.  We then use the plot methods for the cropped ASO `xarray.DataArray` and SnowEx `geopandas.GeoDataFrame`.  The SnowEx data are in WGS-84 but the ASO data are in UTM Zone 12 N.  We use the Geopandas `to_crs` with the CRS for the ASO data accessed using the `rioxarray` accessor for the crs attribute.  This avoids having to hard-code information and, hopefully, avoids mistakes.\n",
    "\n",
    "To distinguish the ASO snow depth raster from the GPR snow depth points we use the Viridis colormap but reverse it for the GPR data.  The idea here is that similar snow depths have high contrast, whereas dissimilar snow depths have low contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "aso_cropped.band_data.plot(ax=ax, vmin=vmin, vmax=vmax, \n",
    "                           cmap=\"viridis\",\n",
    "                           cbar_kwargs={\"label\": \"ASO [m]\"})\n",
    "\n",
    "snowex_gpr.to_crs(aso_cropped.rio.crs).plot('Thickness', ax=ax, s=5, \n",
    "                                            vmin=vmin, vmax=vmax,\n",
    "                                            cmap=\"viridis_r\",\n",
    "                                            legend=True,\n",
    "                                            legend_kwds={\"label\":\"Snowex GPR [m]\"}); #, edgecolor='0.25')\n",
    "ax.set_title(\"Airborne lidar and GPR snow depths\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare ASO and GPR snow depths along the survey transect\n",
    "\n",
    "We can also compare ASO Lidar and SnowEx GPR measurements along the GPR transect in two ways.  First as a plot of snow depths along a transect.  Second with a scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we extract the ASO data that corresponds to the GPR measurement points.  The GPR points and ASO grid do not match exactly, so we interpolate from the ASO grid points to the GPR measurement points.\n",
    "\n",
    "We use _vectorized_ indexing to select data that correspond to the SnowEx GPR points by passing `x` and `y` coordinates as `xarray.DataArray` objects.  `xarray.interp` interprets this input as selecting only the `(x,y)` points.  If we passed `x` and `y` as lists or `numpy.arrays`, interp would return a 2D surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xr.DataArray(snowex_gpr.to_crs(aso_cropped.rio.crs).geometry.x)\n",
    "y = xr.DataArray(snowex_gpr.to_crs(aso_cropped.rio.crs).geometry.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPR coordinates do not exactly match the grid coordinates of 3 m resolution ASO data.  With such high resolution gridded data, it seems reasonable to interpolate the ASO snow depths to the GPR coordinates.  We use the `xarray.Dataset.interp` method to do this.  `xarray.Dataset.interp` is a wrapper for [`scipy.interpolate.interpn`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interpn.html#scipy.interpolate.interpn).  We could use any one of several interpolation methods but choose the `linear` (bilinear in this case) method.  An alternative approach would be to extract snow depth for the nearest ASO grid point.  We use this \"nearest-neighbor\" approach to extract MODIS data below.\n",
    "\n",
    "The interpolation produces a 1D dataset of ASO snow depths for the GPR survey points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aso_transect = aso.interp(x=x, y=y, method='linear')\n",
    "aso_transect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add the ASO snow depth data to the `snowex_gpr` `GeoDataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowex_gpr[\"ASO\"] = aso_transect.band_data.to_pandas()\n",
    "snowex_gpr[[\"date\",\"long\",\"lat\",\"Thickness\",\"SWE\",\"ASO\"]].head()  # Just show coordinates and snow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowex_gpr[[\"Thickness\", \"ASO\"]].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare the snow depths on a scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(snowex_gpr.Thickness, aso_transect.band_data, c='0.25', s=2, alpha=0.5)\n",
    "ax.set_xlabel('GPR (m)')\n",
    "ax.set_ylabel('ASO (m)')\n",
    "ax.set_xlim(0,3)\n",
    "ax.set_ylim(0,3)\n",
    "ax.set_aspect('equal')\n",
    "ax.axline((0.,0.), slope=1., c='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot MODIS Snow\n",
    "\n",
    "Now, let's take a look at the MODIS data.  We want to explore snow cover fraction.  In the MOD10A1 dataset, snow cover fraction as a percentage is calculated from NDSI and stored in the `NDSI_Snow_Cover` variable.  By clicking on the file icon on the row for this variable in the dataset view below, we can see that the data variable doesn't just contain snow cover fraction but also has coded data values for missing data and other quality flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot snow cover fraction for the MODIS image over the western USA.  We use a combination of `matplotlib` and `cartopy`.  I use the Albers Equal Area projection with projection parameters for the contiguous USA.\n",
    "\n",
    "MODIS data are in the [MODIS Sinusoidal Grid](https://modis-land.gsfc.nasa.gov/GCTP.html).  This uses a Sinusoidal projection, which a pseudocylindrical equal area projection.  To plot the data correctly using `cartopy`, we need to define the CRS for the MODIS Sinusoidal projection.  We can access the CRS for the data using the `rioxarray` accessor.  Here, we print this as proj4 string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis.rio.crs.to_proj4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can learn a few things about the MODIS Sinusoidal projection from this.  The `+lon_0=0` tells us that the central longitude is $0\\ ^{\\circ}E$.  `+x_0` and `+y_0` are the false Easting and false Northing, which are both zero.  The `+R=6371007.181` is the semimajor axis of the Spheroid.  You can see a list of Proj4 parameters [here](https://proj.org/en/stable/usage/index.html) \n",
    "\n",
    "`cartopy.crs` has a Sinusoidal projection.  Looking at the Docstring for `cartopy.crs.Sinusoidal`, we can see that the projection uses a default Globe.  The `Globe` object defines the datum and ellipsoid used for the CRS and projection.  Looking at the [cartopy documentation for [`cartopy.crs.Globe`](https://scitools.org.uk/cartopy/docs/latest/reference/generated/cartopy.crs.Globe.html) the default ellipse is WGS84.  So we can't use the `cartopy.crs.Sinusoidal` projection _out-of-the-box_, we have to create a projection using the projection parameters for the MODIS Sinusoidal projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_projection = ccrs.Sinusoidal(\n",
    "    globe=ccrs.Globe(semimajor_axis=modis.rio.crs['R'], ellipse=\"sphere\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To show snow cover fraction and missing data, we use color normalization to map only the values between 0.001 and 100 to the Blues colormap.  We then use the Colormap object to set values less than 0.001% to transparent.\n",
    "\n",
    "```\n",
    "p.axes.cmap.set_under(\"none\")\n",
    "```\n",
    "\n",
    "Values greater than 100 are set to a dark grey to indicate where clouds were detected or where QA was not passed.\n",
    "\n",
    "```\n",
    "p.axes.cmap.set_over(\"0.25\")\n",
    "```\n",
    "\n",
    "To add orientation we add state and country boundaries, along with the coastline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get state boundaries\n",
    "states = cfeature.NaturalEarthFeature(\n",
    "        category='cultural',\n",
    "        name='admin_1_states_provinces_lines',\n",
    "        scale='50m',\n",
    "        facecolor='none')\n",
    "\n",
    "# Set map projection to Albers Equal Area with\n",
    "# projection parameters for contiguous US\n",
    "# From Snyder (https://pubs.usgs.gov/pp/1395/report.pdf)\n",
    "map_proj = ccrs.AlbersEqualArea(\n",
    "    central_longitude=-100., \n",
    "    central_latitude=40., \n",
    "    standard_parallels=(29.5, 45.5)) \n",
    "\n",
    "# Set colormap and normalization\n",
    "norm = Normalize(vmin=0.001, vmax=100)\n",
    "cmap = mpl.colormaps['Blues']\n",
    "# cmap='Blues'\n",
    "\n",
    "p = modis.NDSI_Snow_Cover.plot(\n",
    "    subplot_kws=dict(projection=map_proj),\n",
    "    transform=modis_projection,\n",
    "    norm=norm,\n",
    "    cmap=cmap,\n",
    "    cbar_kwargs={\"extend\": \"neither\", \"orientation\": \"horizontal\", \"label\": \"%\", \"pad\": 0.01},\n",
    ")\n",
    "p.cmap.set_over(\"0.25\")\n",
    "p.cmap.set_under(\"none\")\n",
    "\n",
    "# Add state boundaries\n",
    "p.axes.add_feature(states, edgecolor=\"0.75\")\n",
    "p.axes.add_feature(cfeature.COASTLINE)\n",
    "p.axes.add_feature(cfeature.BORDERS)\n",
    "p.axes.add_feature(cfeature.OCEAN)\n",
    "p.axes.add_feature(cfeature.LAND)\n",
    "\n",
    "p.axes.set_title(\"MODIS Snow Cover Fraction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot MODIS Snow Cover for GPR Survey Region\n",
    "\n",
    "_I am not sure if we use just use this section and delete the preceding section.  If we use just this section, then I will copy some of the text from above here._\n",
    "\n",
    "We want to be able to match MODIS snow cover fraction with the GPR Survey points.  A good first step is to visualize the MODIS data and GPR survey transect.  To do this, we'll clip the MODIS data to the bounding box of the survey data, using a similar approach to clipping the ASO data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the bounding box of the SnowEx GPR data in the MODIS coordinate system.  Then we use this `clip_region` to clip the MODIS snow cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_region = [box(*snowex_gpr.to_crs(modis.rio.crs).total_bounds)]\n",
    "snow_cover_clipped = modis.NDSI_Snow_Cover.rio.clip(clip_region, all_touched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the plot of MODIS data above for the western US, we will use the MODIS Sinusoidal projection for our plot over the GPR region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_proj = modis_projection\n",
    "\n",
    "# Define based on search polygon\n",
    "# coords = roi_polygon_gdf.to_crs(map_proj.to_wkt()).geometry.get_coordinates()\n",
    "# roi_bbox_map = [coords.x.min(), coords.y.min(), coords.x.max(), coords.y.max()]\n",
    "\n",
    "norm = Normalize(vmin=0.001, vmax=100)\n",
    "# cmap = Colormap('Blues')\n",
    "cmap='Blues'\n",
    "\n",
    "# p = modis.NDSI_Snow_Cover.rio.clip(box(*roi_bbox_map)).plot(\n",
    "p = snow_cover_clipped.plot(\n",
    "    subplot_kws=dict(projection=map_proj),\n",
    "    transform=modis_projection,\n",
    "    norm=norm,\n",
    "    cmap=cmap,\n",
    "    cbar_kwargs={\n",
    "        \"extend\": \"neither\", \n",
    "        \"orientation\": \"horizontal\", \n",
    "        \"label\": \"%\", \n",
    "        \"pad\": 0.01},\n",
    ")\n",
    "p.cmap.set_over(\"0.25\")\n",
    "p.cmap.set_under(\"none\")\n",
    "\n",
    "# Add SNOTEL location\n",
    "snowex_gpr.to_crs(map_proj).plot(ax=p.axes, c=\"k\")\n",
    "\n",
    "p.axes.set_title(\"MODIS Snow Cover Fraction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Snow Cover From Modis for GPR Survey\n",
    "\n",
    "We can use a similar approach to the one we used to extract the ASO snow thickness to extract snow cover fraction.  However, in this case we are going to select the values for MODIS pixels nearest to the survey points.\n",
    "\n",
    "We first convert the x and y coordinates of the survey points to the MODIS CRS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xr.DataArray(snowex_gpr.to_crs(modis.rio.crs).geometry.x, dims=[\"point\"])\n",
    "y = xr.DataArray(snowex_gpr.to_crs(modis.rio.crs).geometry.y, dims=[\"point\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we use the `sel` method to extract the nearest data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_snow_cover_point = modis.NDSI_Snow_Cover.sel(x=x, y=y, method=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_snow_cover_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the ASO data, we add the MODIS snow cover as a column to the SnowEx GPR `geopandas.GeoDataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowex_gpr[\"modis_scf\"] = modis_snow_cover_point.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowex_gpr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export SnowEx GeoDataFrame with ASO and MODIS snow cover to Shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `snowex_gpr` dataframe can be exported as a shapefile for further analysis in GIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowex_gpr['date'] = snowex_gpr['date'].apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "snowex_gpr.to_file('snow-data-20170208.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional data imagery services\n",
    "\n",
    "#### NASA Worldview and the Global Browse Imagery Service\n",
    "\n",
    "NASA’s EOSDIS Worldview mapping application provides the capability to interactively browse over 900 global, full-resolution satellite imagery layers and then download the underlying data. Many of the available imagery layers are updated within three hours of observation, essentially showing the entire Earth as it looks “right now.\"\n",
    "\n",
    "According to the [MOD10A1 landing page](https://nsidc.org/data/mod10a1), snow cover imagery layers from this data set are available through NASA Worldview. This layer can be downloaded as various image files including GeoTIFF using the snapshot feature at the top right of the page. This link presents the MOD10A1 NDSI layer over our time and area of interest: https://go.nasa.gov/35CgYMd. \n",
    "\n",
    "Additionally, the NASA Global Browse Imagery Service provides up to date, full resolution imagery for select NSIDC DAAC data sets as web services including WMTS, WMS, KML, and more. These layers can be accessed in GIS applications following guidance on the [GIBS documentation pages](https://wiki.earthdata.nasa.gov/display/GIBS/Geographic+Information+System+%28GIS%29+Usage). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "To cleanup your directory, uncomment and run the cell below.  This will remove the files you have downloaded to the download directory and the shapefile you have saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf download\n",
    "# !rm snow-data-20170208.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
