{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ebd94c-fb02-42a7-9cfe-b02a0da5967a",
   "metadata": {},
   "source": [
    "# Read and plot SMAP data\n",
    "\n",
    "## Overview\n",
    "\n",
    "This has become an overly long notebook.  It was started to demonstrate some more _pythonistic_ streamlined workflows than we show in the SMAP tutorials for accessing, loading and plotting SMAP L3 data.  However, what should have been a short and simple demonstration of workflows became an exploration of the shortcomings of the L3 SMAP HDF5 files and workarounds to deal with them.  What follows is a hybrid of a tutorial, critique and suggested improvements.  I summarize the problems with the data set in this overview section.  Please see the code and plots below for examples.\n",
    "\n",
    "The tutorials use the SMAP L3 Radiometer Global Daily 36 km EASE-Grid Soil Moisture, Version 8 (SPL3SMP, DOI:10.5067/OMHVSRGFX38O).\n",
    "\n",
    "[Data set landing page](https://nsidc.org/data/spl3smp/versions/8)  \n",
    "[User Guide](https://nsidc.org/sites/default/files/spl3smp-v008-userguide.pdf)\n",
    "\n",
    "### Problems with the L3 SMAP Data\n",
    "\n",
    "This list is not exhaustive.  It is informed by trying to describe workflows to open, read and plot the data contained in the files using current, standard tools in the geospatial python stack.  I suspect that the same problems and frustrations would be encountered if other software tools and programming languages were used.\n",
    "\n",
    "The critique is also inspired by the belief that self-desribing file formats, conventions such as CF and COARDS, along with software tools designed to take advantage of these file formats and conventions, are there to make the working with data simple, allow scientists to start analysis sooner.  Ad-hoc file structures and incomplete (or inaccurate) metadata _\"break\"_ these tools, workflows.  Often the _forensic investigations_ required to track down the necessary information to \"fix\" problems introduced by non-conforming files is time consuming.  The following quote from the CF Conventions makes this point well.\n",
    "\n",
    ">The purpose of the CF conventions is to require conforming datasets to contain sufficient metadata that they are self-describing in the sense that each variable in the file has an associated description of what it represents, including physical units if appropriate, and that each value can be located in space (relative to earth-based coordinates) and time.\n",
    ">\n",
    ">An important benefit of a convention is that it enables software tools to display data and perform operations on specified subsets of the data with minimal user intervention. It is possible to provide the metadata describing how a field is located in time and space in many different ways that a human would immediately recognize as equivalent. The purpose in restricting how the metadata is represented is to make it practical to write software that allows a machine to parse that metadata and to automatically associate each data value with its location in time and space. It is equally important that the metadata be easy for human users to write and to understand.\n",
    "\n",
    "1.  **The HDF5 files do not contain sufficient information for the data to be geolocatable**.  The data are in a projected coordinate reference system (CRS) - EASE-Grid 2.0 - on a 406 x 964 grid with a 36 km grid cell height and width.  Although `latitude` and `longitude` 2D grids are included as Datasets, these grids are masked, so that coordinates are not available for all grid cells.  Coordinate grids **should not** have missing values.  Furthermore, standard tools in the Geoscience stack, such as `xarray`, do not allow coordinate variables to have missing data in accordance with conventions such as CF and COARDS.\n",
    "2.  The data have a projected CRS, the projected coordinates should be included in the file.  These would be vectors containing `x` and `y` in meters.\n",
    "3.  There are no named dimensions.  Standard tools (and humans) have no way to associate dimension variables (dimension scales in HDF5 terms) with data variables.\n",
    "4.  There is no information about the CRS contained in the file.  It may be in the xml but modern tools expect to find it as a variable.  It should be included at least as a WKT string and an EPSG code.\n",
    "5.  The names of Datasets have an inconsistent naming convention.  Datasets in the `Soil_Moisture_Retrieval_Data_PM` Group have the suffix `_pm`.  Datasets in the `Soil_Moisture_Retrieval_Data_AM` group do not.  This makes programmatic workflows more complicated because users have to \"code around\" the differences in file names.  For example: `if spass == 'PM': filename = f\"{variable}_{spass.lower()} \\\\ else: filename = variable` instead of `filename = f\"{variable}_{spass.lower()}\"`.\n",
    "6.  Some of the attribute data is incorrect.  See the `landcover_class` for an example.  The `long_name` attribute is used to describe the dimensions.  This is the wrong place to describe this.  `long_name` should be reserved for the name of the variables.  If named dimensions had been created there would be no need to describe the dimensions.  Moreover, dimensions are listed as longitude, latitude and IGBP land cover type.  Longitude and latitude cannot be dimensions.  The data arrays are in projected coordinates.  Longitude and latitude are 2D arrays and, therefore, it is impossible to assign them as dimensions, which by definition, are 1D vectors.  The dimensions should be x and y or easting and northing. \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a4034-033a-419c-97a1-fb3a4234e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import earthaccess\n",
    "import h5py\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "from affine import Affine\n",
    "from pyproj import CRS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a65b4d-42a7-49b7-807f-150daf3928e5",
   "metadata": {},
   "source": [
    "## Search for SMAP data\n",
    "\n",
    "We will use `earthaccess` to search for SMAP Level 3 files for March 2017.  The first step is to authenticate using `earthaccess.login`.  We can then use `search_data` to search for data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c73ccc-c8dc-4dcb-84bf-962f8b0e6c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = earthaccess.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04c8b8-fe00-4452-9989-8a32012ed338",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    short_name = 'SPL3SMP',\n",
    "    version = '009',\n",
    "    cloud_hosted = False,\n",
    "    temporal = ('2017-03-01','2017-03-31')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8b84a6-6085-40a1-bebf-792ad2605549",
   "metadata": {},
   "source": [
    "## Download files\n",
    "\n",
    "If you haven't already, we'll download the files to a local directory.  This is defined in the `DOWNLOAD_PATH` constant.  In this example, we'll use the `Path` object from the [`pathlib`](https://docs.python.org/3/library/pathlib.html) package to build a path to a directory `smap_data` under the current working directory.  `Path` objects have methods to return the current working directory `.cwd()`, make directories `.mkdir()`, and search using globbing `.glob()`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> If you want to download files to a different directory change the cell below to  \n",
    "\n",
    "```\n",
    "DOWNLOAD_PATH = Path(\"path-to-download-directory\")\n",
    "```\n",
    "</div>\n",
    "\n",
    "`earthaccess.download` checks if the files already exist in the local download directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f11521-54ff-4dd0-8823-cea9d8bba6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rm smap_data/*.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7cb4e8-0854-4dfa-b2a0-28fffca94305",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_PATH = Path.cwd() / \"smap_data\"\n",
    "DOWNLOAD_PATH.mkdir(parents=True, exist_ok=True)  # creates parents if they don't exist and fails silently if path exists\n",
    "\n",
    "smap_files = earthaccess.download(results, str(DOWNLOAD_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2696e5a4-92be-4860-a84b-7ea387ea25bf",
   "metadata": {},
   "source": [
    "I then use `.glob('*.h5')` to get a list of SMAP files in the `DOWNLOAD_PATH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a474c513-0473-43eb-b2d1-f386c52272b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smap_files = list(DOWNLOAD_PATH.glob('*.h5'))  # .glob() returns a generator list() turn it into a list \n",
    "smap_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f49af27-5890-4b0e-807c-d7394594d008",
   "metadata": {},
   "source": [
    "## Read data from HDF5 file using `h5py`\n",
    "\n",
    "HDF5 files are hierarchical data structures that consist of _Groups_ and _Datasets_.  Groups contain sub-groups and Datasets.  Datasets are equivalent to Variables in NetCDF files.  Both Groups and Datasets can have attributes.  Datasets are multidimensional arrays similar to numpy arrays and have `shape`, `ndim`, `dtype` attributes the same as numpy arrays.\n",
    "\n",
    "The best way to explore an HDF5 file is to use a \"viewer\".  [MyHDF5](https://myhdf5.hdfgroup.org/) is one such tool.  You can upload a file from your local machine or point to a URL for a file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea9670-0d39-4ab2-bc10-bcfb2d05fa46",
   "metadata": {},
   "source": [
    "We open an HDF5 file using `h5py.File`, which returns a file object.  This file object is also the _root_ group for the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2113e065-5aa3-4a9f-a537-170cd92e69ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = h5py.File(smap_files[0], 'r')\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16419b2-304b-4d38-978c-e47ba23d9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "root.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f263a-0233-42e4-a904-658c94a7cb07",
   "metadata": {},
   "source": [
    "_Add examples of accessing Groups by path, accessing attributes and data arrays, as well as dimensions_\n",
    "\n",
    "A HDF5 `Dataset` can be accessed using a similar syntax to how you would access a file on your hard drive.  For example we can access the `soil_moisture` data set for the AM pass with the following command.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf41372e-df90-4a42-adf4-ebab4b210909",
   "metadata": {},
   "outputs": [],
   "source": [
    "root['Soil_Moisture_Retrieval_Data_AM/soil_moisture']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4b57b9-593e-4634-9c85-91fd150e4c72",
   "metadata": {},
   "source": [
    "We can see that this is a data array with shape 406 x 964 containing 32 bit floating point numbers (`<f4` indicates 4-byte float).  This information can also be accessed using the `name`, `shape`, `ndim` and `dtype` attributes of the `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7bc640-ab04-45c0-a6d8-b236e8a0f012",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(root['Soil_Moisture_Retrieval_Data_AM/soil_moisture'].name)\n",
    "print(root['Soil_Moisture_Retrieval_Data_AM/soil_moisture'].shape)\n",
    "print(root['Soil_Moisture_Retrieval_Data_AM/soil_moisture'].ndim)\n",
    "print(root['Soil_Moisture_Retrieval_Data_AM/soil_moisture'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0752bac-44bc-423e-82ff-2dc67faff324",
   "metadata": {},
   "source": [
    "The `soil_moisture` `Dataset` has two dimensions with sizes 406 and 964.  HDF5 allows dimensions to have a name and have an attached scale.  For geoscience data, the logical scales attached to a dimension are coordinates.  This not only provides context for the data, making it richer and more useable, it also georeferences the data in space.  Without coordinates the grids are not locatable on Earth.\n",
    "\n",
    "Unfortunately, none of the Datasets in the L3 SMAP soil moisture files have named dimensions or attached scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e6003-3894-42f0-acec-f5d69ba6c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in root['Soil_Moisture_Retrieval_Data_AM/soil_moisture'].dims[0].items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431007e-f12a-46f7-9e7e-aeaa93eb4271",
   "metadata": {},
   "source": [
    "To access the data array and write it to a variable, we use the following syntax.  Even though the data is a two dimensional array, we can get all the data with `[:]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6afd80d-388b-48f8-bc62-38af3c8f4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = root['Soil_Moisture_Retrieval_Data_AM/soil_moisture'][:]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b704e3-5f25-41df-a19c-8780b1e1e1c2",
   "metadata": {},
   "source": [
    "To access a slice of the data array, we can use the normal syntax for slicing a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07409908-e903-416b-a574-da918c914daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "root['Soil_Moisture_Retrieval_Data_AM/soil_moisture'][0:4, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be7190c-8c1f-41de-ac3b-6f1dcc122860",
   "metadata": {},
   "source": [
    "The `Dataset` attributes can be accessed using the `attr` attribute.  This returns a dictionary-like object that can be accessed using `keys`, `values` and `items` methods, along with `h5py` package specific methods.  Here, we use a `for` loop and `items` to iterate through and print the key:value pairs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> The non-numerical data attributes are byte strings, these can be converted to strings using `decode`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc57f9e-6f08-460f-b15b-eadfde6d3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in root['Soil_Moisture_Retrieval_Data_AM/soil_moisture'].attrs.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b8ff0-e329-4505-a8e1-3e607a9887ea",
   "metadata": {},
   "source": [
    "## _Optional_:  An alternative way to access Groups and Datasets\n",
    "\n",
    "An alternative way to list `Groups` and `Datasets` in the file is to use the `visit` and `visititems` methods.  Both these methods iterate over the objects in a file. They take a function name as an argument.  `visit` takes a function with the form:\n",
    "\n",
    "```\n",
    "myfunc(<member name>)\n",
    "```\n",
    "\n",
    "This function can be used to return a list of all Groups and Datasets in the file.  In the example below we create an empty list and then pass the `list.append` method, which is a function, to visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d27b73-0d8b-4ebe-baed-c3c5324836ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(smap_files[0], 'r') as root:\n",
    "    list_of_names = []\n",
    "    root.visit(list_of_names.append)\n",
    "list_of_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e44f69e-1dd6-44eb-9aaf-744aeb888f76",
   "metadata": {},
   "source": [
    "We can also write a function to look for members with albedo in the name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e1bd7-07ee-4609-9943-cec5ca527a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(smap_files[0], 'r') as root:\n",
    "    soil_moisture = root['Soil_Moisture_Retrieval_Data_AM/soil_moisture'][:]\n",
    "soil_moisture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06263ffa-d2fb-4e12-86d3-e2cfff25677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_names = []\n",
    "def get_albedo(name):\n",
    "    if 'albedo' in name.lower():\n",
    "        list_of_names.append(name)\n",
    "\n",
    "root.visit(get_albedo)\n",
    "list_of_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc5739-e32f-42e0-af28-55e66bf6eebe",
   "metadata": {},
   "source": [
    "`visititems` not only takes a function with the member name but also the actual Group or Dataset object.\n",
    "\n",
    "```\n",
    "myfunc(name, obj)\n",
    "```\n",
    "\n",
    "we can use this to get particular variables or do some form of processing based on whether the object is a Group or a Dataset.\n",
    "\n",
    "In the example below, we will extract the data arrays for albedo variables for the AM pass.  I first look for `AM` in the name and then check if the name contains `albedo` as in the `visit` example above.  I only want `Datasets`.  `Groups` do not contain data arrays.  To do this, I use the `isinstance` function that is part of the standard library.  `isinstance` take `obj` as an argument and checks to see if it is of the type `h5py.Dataset`.\n",
    "\n",
    "If all three of these conditions are `True`, I then want to write the variables to a dictionary.  I want to use the name of the `Dataset` as the key for each data array.  I can split the `name` string on the `/` character and take the last element of the list that is returned.  To assign the data array and not the `Dataset` object, I use the `[:]` syntax.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e3b4f-5283-4d1d-96e1-965a623e361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_vars = {}\n",
    "def get_vars(name, obj):\n",
    "    if ('AM' in name) & ('albedo' in name) & isinstance(obj, h5py.Dataset):\n",
    "        dict_of_vars[name.split('/')[-1]] = obj[:]\n",
    "\n",
    "root.visititems(get_vars)\n",
    "dict_of_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084a4e87-ad0b-4379-af68-bfd8faf5f9bc",
   "metadata": {},
   "source": [
    "The `h5py.File` opens the file.  The file needs to be closed, otherwise it remains in memory.  If you are reading multiple files, this could lead to memory problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded43124-14cc-4be6-b690-be4f7f3bcceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "root.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304aa23-5196-4c6b-b92d-d2ed050f3fe0",
   "metadata": {},
   "source": [
    "## Creating a georeferenced `xarray.Dataset` for a subset of variables from an HDF5 SMAP file\n",
    "\n",
    "Fully georeferenced data sets are required for Earth science analysis workflows.  This may be as simple as plotting a map.  You may want to transform the data so it can be used with another data set, or you may want to perform some geospatial subsetting.\n",
    "\n",
    "Unfortunately, the SMAP HDF5 files do not contain sufficient information to geolocate data on the Earth surface.  This is is the case both for using the data with software tools to display or manipulate the data, and for humans trying to understand the data.  The following example shows:\n",
    "\n",
    "1. how to read a subset of `Datasets` (variables) from the HDF5 file and create a geospatially aware `xarray.Dataset`;\n",
    "2. how to mask missing data;\n",
    "3. how to add coordinate information to the dataset;\n",
    "4. how to add a coordinate reference system information the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa714ea3-2dd2-4831-b482-a0421e0979f9",
   "metadata": {},
   "source": [
    "`xarray` can read a group from an HDF5 file.  In the following example, we read the `Soil_Moisture_Retrieval_Data_AM` group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e1df8-eb4f-472d-8175-69e634c30503",
   "metadata": {},
   "outputs": [],
   "source": [
    "gds = xr.open_dataset(smap_files[0], group='Soil_Moisture_Retrieval_Data_AM')  #, decode_coords='all', decode_cf=True)\n",
    "gds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d558e4-448d-4c7a-a188-ea5cf27dc124",
   "metadata": {},
   "source": [
    "53 variables are loaded from `Soil_Moisture_Retrieval_Data_AM`.  However, we can immediately see some issues.  The dimensions are named `phony_dim_0`, `phony_dim_1`, and `phony_dim_2`, which is not much help.  This is because no dimensions are assigned to Datasets so `xarray` has to generate some dummy variables.  The same problem arises if the Python binding for HDF5 library, `h5py`, were used.\n",
    " \n",
    "Let's change the names of the dimensions to something useful and meaningful.  The first thing is to figure out what the dimensions are.  This is more diffcult than is should be.  Some of the metadata is wrong.  Some is confusing.  The description of the `landcover_class` offers some information, albeit misleading, but it is a clue.\n",
    "\n",
    "> An enumerated type that specifies the most common landcover class in the grid cell based on the IGBP landcover map. The array order is longitude (ascending), followed by latitude (descending), and followed by IGBP land cover type descending dominance (only the first three types are listed)\n",
    "\n",
    "This description states that the dimensions of the landcover_class array are longitude, latitude and IGBP land cover class.  The data are in projected coordinates (EASE Grid 2.0) **so longitude and latitude cannot be dimensions**.  Longitude and latitude must be 2D arrays for a projected dataset.  However, x and y or easting and northing are likely candidates.  However, the 3rd dimension appears to be land cover class.  Plotting the `soil_moisture` variable also shows that `phony_dim_1` is the x-dimension and `phony_dim_0` is the y-dimension in cartesian coordinates.  So we'll rename the coordinates:\n",
    "\n",
    " - `phony_dim_0 --> y`\n",
    " - `phony_dim_1 --> x`\n",
    " - `phony_dim_2 --> igbp_class`\n",
    "\n",
    "We can do this using `rename_dims`, which takes a dictionary as an argument, where keys are the old dimension name and values are the new dimension name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a445433e-d160-4ca2-9a58-a0b74ccf348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gds = gds.rename_dims({'phony_dim_0': 'y', 'phony_dim_1': 'x', 'phony_dim_2': 'igbp_class'})\n",
    "gds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de673df1-483f-47b0-9e9a-c1aadd711629",
   "metadata": {},
   "source": [
    "Another issue is that there are no coordinate variables, again because the HDF5 file was not created with any.  Inspecting the variables, we see that there are two possible candidate coordinate variables: `latitude` and `longitude`.  So let's plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f26e1e-2c6d-4f57-bf51-72653e2c76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.latitude.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9bc187-b64d-414d-9074-5bc5e33c3774",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gds.latitude.min(), gds.latitude.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba404a65-2dd2-4f1b-a538-40861dfcd199",
   "metadata": {},
   "source": [
    "If we plot both latitude and soil_moisture, we can see that latitude values have been masked with -9999. where there are no valid data values.  **This is not a good or sensible approach because latitude and longitude are coordinates.  As long as latitude and longitude values are between -90 and 90, and -180 and 180 (or 0 and 360), they are valid coordinates.  Under NetCDF and CF conventions, coordinate values should not have missing values.  We also need coordinates for all grid cells for plotting and other operations.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8326c-fea7-4fb5-a7bc-e38f4bbfdd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1)\n",
    "gds.latitude.plot(ax=ax[0]);\n",
    "gds.soil_moisture.plot(ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5222c-82ba-4c58-83b5-0b13c1919ec2",
   "metadata": {},
   "source": [
    "Neither the plot of latitude nor the minimum and maximum values look as we would expect.  We expect latitude to be in the range of -90 to +90.  The maximum is actually in this range but the minimum is -9999., which is suspiciously like a `_FillData` or missing data value.  We would also expect the image to show a smooth field that only varies in the vertical direction, e.g. from -90. to 90. or some similar range.  Instead it shows a set of lozenges.\n",
    "\n",
    "Let's set the `_FillValue` attribute for latitude to `-9999`.  In `xarray`, we have to use `where` to apply a mask everywhere a condition is not `True`.  Here we set the condition to be _not equal to -9999._  To save time and errors later, we also set the `encoding` entry for `fillvalue` to -9999.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "`encoding` is a dictionary of parameters that defines how data are \"encoded\" in the file.  It includes information about how data are compressed, chunked, the _FillValue, and data type.  Saving the `encoding` is not strictly necessary unless you want to write the data to a new file.  We'll follow these steps here to demonstrate how to preserve the encoding data. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65618de-eb3d-4f07-804c-81f0199a3ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gds.latitude.encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc06b5a-fe82-4732-8753-f589485bb3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fillvalue = -9999.\n",
    "\n",
    "# `where` does not preserve encoding, so we have to copy first\n",
    "encoding = gds.latitude.encoding\n",
    "# Apply fillvalue mask\n",
    "gds['latitude'] = gds.latitude.where(gds.latitude != fillvalue)\n",
    "# we copy the encoding back to DataArray\n",
    "gds.latitude.encoding = encoding\n",
    "# ...and update encoding to include fillvalue\n",
    "gds.latitude.encoding['fillvalue'] = fillvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b2d12-c6ce-4fc4-b924-d5b3e4b13cf9",
   "metadata": {},
   "source": [
    "Now when we check the range of the latitude values, we see that the range is between -83.63... and 83.63... as we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec868a8e-bf13-4967-bde6-02cc2b07738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gds.latitude.min(), gds.latitude.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acf9bca-d60d-4d34-9618-7d33e7236772",
   "metadata": {},
   "source": [
    "Plotting the data again, we see that latitude values are within the expected range but that hasn't solved the missing data problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d6df08-b03f-465a-bb21-d6a68665fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1)\n",
    "gds.latitude.plot(ax=ax[0]);\n",
    "gds.soil_moisture.plot(ax=ax[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efc6edf9-0555-4256-86c7-c6527c9746c7",
   "metadata": {},
   "source": [
    "#### Adding coordinates\n",
    "\n",
    "There are no useful coordinate values.  If we plotted `longitude` we would see the same problem, and beside we need both latitude and longitude.  As noted above, the files do not include complete information about the coordinate reference system or the grid definition.  So we need to do some data sleuthing.\n",
    "\n",
    "Tables 4 and 5 in the [User Guide](https://nsidc.org/sites/default/files/spl3smp-v008-userguide.pdf) provide geospatial information.  Further information on EASE Grids can be found on [A Guide to EASE Grids page](https://nsidc.org/data/user-resources/help-center/guide-ease-grids).\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> The ATBD for the L3 SMP data gives the data array dimensions as 406 x 963.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d642bf9-c8fb-431d-8467-7720a3378081",
   "metadata": {},
   "source": [
    "The CRS and grid definition are independent.  The CRS defines how projected coordinates, usually in meters, relate to geographic latitude and longitude.  The grid definition relates how image coordinates (columns and rows) map to the projected coordinates.  The Guide to EASE Grids has 8 grid definitions, ranging from 1 km to 36 km cell sizes, for the Global EASE-Grid 2.0 CRS (EPSG:9633).  We could define many more grid definitions.  For more information on CRS, map projections and grids see [_Link to be added_]().\n",
    "\n",
    "In the [table](https://nsidc.org/data/user-resources/help-center/guide-ease-grids#anchor-10) for the 36 km Global EASE-Grid 2.0 we are given the height and width of each grid cell, and the x and y coordinates of the upper-left corner of the upper-left grid cell.  We can see that the 36 km grid cell size is not exact.  In fact the value the table gives is not exact.  The actual value runs to 9 decimal places.  This is because the grid cell height and width of the global grid were defined so that an exact number of cells span the equator {@brodzik_ease}.  This is not necessary for polar grids.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Exact definitions of CRS, projection and grid parameters can be found in the Grid Parameter Definition (gpd) files in the [`mapxmaps` GitHub repository](https://github.com/nsidc/mapxmaps).  These gpd files have a somewhat esoteric format, so might take a little bit to understand.  It is important that the grid definition is exact because we cannot merge datasets if the coordinates do not match, even to the 9th decimal place.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc5886-d10a-4ca7-b2df-e10ac1de0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cell_width = 36032.220840584\n",
    "grid_cell_height = -36032.220840584\n",
    "x_upper_left_corner = -17367530.4451615\n",
    "y_upper_left_corner = 7314540.8306386"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e8b99-28d0-46d0-9c0e-8f9fd6223861",
   "metadata": {},
   "source": [
    "The projected coordinates of a grid cell can be found using the following formulas:\n",
    "\n",
    "$$\n",
    "x = (grid\\_cell\\_width \\times row) + x\\_upper\\_left\\_corner\n",
    "$$\n",
    "$$\n",
    "y = (grid\\_cell\\_height \\times column) + y\\_upper\\_left\\_corner)\n",
    "$$\n",
    "\n",
    "This equations assumes that the origin of the image coordinates start at row=0. and column=0. in the upper-left corner of the upper-left grid cell.  So the center of the upper-left grid cell is row=0.5 and column=0.5.  Adding 0.5 to row and column indices solves this problem.\n",
    "\n",
    "We could write a function to return (x, y) coordinates for (column, row) image coordinates using the equations above.  Or we can use the `Affine` class from the `affine` package to define an affine matrix.  This is what we'll do below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed97dc-b521-4240-9446-8811c8694aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Affine(grid_cell_width, 0.0, x_upper_left_corner, \n",
    "                   0.0, grid_cell_height, y_upper_left_corner)\n",
    "transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08669cb5-4848-41e7-a08b-0b3e71d21f40",
   "metadata": {},
   "source": [
    "This then allows us to calculate x and y coordinates from column and row indices using matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bd706e-78fc-4cc5-a11a-ff0cb22ca93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform * (964, 406)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fc024-b032-406a-acdc-f7a76dea9e49",
   "metadata": {},
   "source": [
    "We'll define vectors for the row and column indices of the grid cell centers, and then use the `transform` matrix to get x and y coordinates.  These only need to be vectors as well because each column will have the same row indices and each row will have the same column indices. We'll use the `numpy.arange` function to do and start the range at 0.5 to offset the row and column indices for grid cell centers by 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a314543-1a0c-482d-86d3-5905b6b11b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncolumns, nclass = gds.dims['y'], gds.dims['x'], gds.dims['igbp_class']  #data_vars['soil_moisture'][1].shape\n",
    "row = np.arange(0.5, nrows)\n",
    "column = np.arange(0.5, ncolumns)\n",
    "\n",
    "x, _ = transform * (column, 0.5)\n",
    "_, y = transform * (0.5, row)\n",
    "igbp_class = np.arange(nclass)  # Add attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c7ee2c-05e7-4f00-a30c-e8697c91499e",
   "metadata": {},
   "source": [
    "We need to assign `x`, `y`, and `igbp_class` as coordinates.  This should be sufficient for plotting.  However, if we want to reproject the data we should define the CRS and spatial coordinates, and transform.  We do this using `rioxarray` accessors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda308a-6433-4174-b14e-b162a71ae729",
   "metadata": {},
   "outputs": [],
   "source": [
    "gds = gds.assign_coords(x=x, y=y, igbp_class=igbp_class)\n",
    "\n",
    "gds.rio.write_crs(6933, grid_mapping_name='wgs84_easegrid2_global', inplace=True)\n",
    "gds.rio.set_spatial_dims(x_dim='x', y_dim='y', inplace=True)\n",
    "gds.rio.write_coordinate_system(inplace=True)\n",
    "gds.rio.write_transform(transform)\n",
    "gds.rio.write_grid_mapping('wgs84_easegrid2_global', inplace=True)\n",
    "gds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb574a-7f44-4924-b275-0fce25606569",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gds.latitude.rio.grid_mapping)\n",
    "print(gds.rio.bounds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f54aaa-a1fc-46a2-826d-737bd71aa386",
   "metadata": {},
   "outputs": [],
   "source": [
    "EASEGrid2 = ccrs.epsg(gds.rio.crs.to_epsg())\n",
    "\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "ax = fig.add_subplot(projection=EASEGrid2)\n",
    "\n",
    "ax.coastlines()\n",
    "gds.soil_moisture.plot(ax=ax,)\n",
    "ax.set_title('Soil Moisture');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae67f0aa-b858-4ee1-aa29-3961df9c54cc",
   "metadata": {},
   "source": [
    "## Extra Stuff to be refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d00eaa2-7b6e-477a-b71c-6d90d09aa0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_mapping_name = \"WGS84_NSIDC_EASEGrid_2.0_Global\"\n",
    "datasets = ['Soil_Moisture_Retrieval_Data_AM/soil_moisture', 'Soil_Moisture_Retrieval_Data_AM/retrieval_qual_flag']\n",
    "data_vars = {}\n",
    "dims = ['y', 'x']\n",
    "with h5py.File(smap_files[0]) as root:\n",
    "    for dataset in datasets:\n",
    "        ds = root[dataset]\n",
    "        path = ds.name\n",
    "        name = path.split('/')[-1]\n",
    "        attrs = {k: (v.decode('UTF-8') if hasattr(v, 'decode') else v) for k, v in dict(ds.attrs).items()}\n",
    "        attrs['grid_mapping'] = grid_mapping_name  # Add so that dataset is CF-compliant and geoferenced\n",
    "        data_vars[name] = (dims, ds[:], attrs)\n",
    "\n",
    "data_vars\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39afd220-8f13-4993-a25a-5ec0513476be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data_vars['soil_moisture'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7878b4-eec8-4214-8479-df2a8aab1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vars['soil_moisture'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5329fb83-8b95-42db-8ac1-7efe555a0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(x, y, data_vars['soil_moisture'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d7efa-0fb3-418b-93b6-04132a0375fe",
   "metadata": {},
   "source": [
    "We also need to define the CRS.  We know that the CRS has the EPSG code 6933.  We can use `pyproj` to get the CRS definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbd0150-c72c-453b-8045-489f702b8349",
   "metadata": {},
   "source": [
    "Base on [CF-Conventions](http://cfconventions.org/cf-conventions/cf-conventions.html#grid-mappings-and-projections), we add the CRS as a variable with the same name as we used for the `grid_mapping` attribute of each data variable.  We define this variable as an integer.  The grid mapping information is then copied as attributes for that variable.  We use the `to_cf()` method of the `pyproj.CRS` object to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037dc012-8f52-4144-b929-fb3169dcebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset(data_vars, coords={'x': x, 'y': y})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667dbefd-1a3e-4b9f-94a7-dfc0cb314961",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.rio.write_crs(6933, inplace=True)\n",
    "\n",
    "#ds[grid_mapping_name].attrs = crs.to_cf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58167732-336b-49a1-b915-2b3683336226",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds =xr.decode_cf(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f479de98-57b8-401d-8ae6-f5372c835dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.soil_moisture.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98fef70-b2eb-4e17-90f1-58d8e93c2f56",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
